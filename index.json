[{"categories":["SDDC"],"content":"I‚Äôve talked about why everyone needs a homelab in my medium article but never had the courage to submit my homelab to the community homelab list maintained by William Lam Follow. Mainly because I wasn‚Äôt sure how my lab would stack up against everyone elses' submission. With my latest post covering my experience enabling Tanzu with vSphere in my homelab, I feel like I‚Äôve gone through the checklist of best practices and I‚Äôm proud of my minimalist(if i can say so myself) homelab setup! Self Congrats!\" Self Congrats! ","date":"2021-02-15","objectID":"/post/homelab-2021/:0:0","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"Lab BOM In this post, I‚Äôm going to provide an overview of what I‚Äôve installed, cost of ownership, cost of operations and use cases ","date":"2021-02-15","objectID":"/post/homelab-2021/:1:0","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"Hardware 1x 15U Open Frame rack - approx $200 on ebay 2x Dell R620 - approx $350(for RAM) + $100(for CPUs) + $400(for servers) (unsupported CPU for vSphere 7.0) CPU: 2x E5-2630 v2 Memory: 128GB RAM Storage: Model: PERC H310 Mini (for monolithic) 2x 500GB HDD 2x Dell R730 - approx $815(for RAM) + $100(for SSDs) + $950(for servers) CPU: 2x E5-2620 v3 Memory: 128GB RAM Storage: Model: PERC H730 Mini 2x 500GB HDD 2x 100GB SSD 1x Dell R720XD - approx $750 CPU: 2x Intel¬Æ Xeon¬Æ E5-2670 Memory: 92GB RAM Storage: Model: PERC H710 12x 3.5\" 3TB HDD 2x 2.5\" 1TB SSD Cables, UPS and Accessories - Under $500 ","date":"2021-02-15","objectID":"/post/homelab-2021/:1:1","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"Networking TP-Link (an old leftover router) routes to my ISP through a firewall Cisco 2960 Switch (mainly for VLANs) 10.10.10.0/24 - (VLAN 10) Management 10.10.20.0/24 - (VLAN 20) VM 10.10.30.0/24 - (VLAN 30) vMotion 10.10.50.0/24 - (VLAN 50) vSAN 10.10.60.0/24 - (VLAN 60) NSX Overlay/TEP 10.10.70.0/24 - (VLAN 70) NSX Uplink default route back to TP-Link ","date":"2021-02-15","objectID":"/post/homelab-2021/:1:2","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"Total Cost Hardware: approx $4000 Electricity: under 50$ pm ","date":"2021-02-15","objectID":"/post/homelab-2021/:1:3","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"SDDC Setup Mgmt: ESXi 6.7 on 2x R620 vCenter 7.0 U1 GA 2x NSX Manager 2x NSX Edge 1x VSAN Witness Compute: ESXi 7.0 GA on 2x R730 2 node vSAN Cluster enabled NSX Overlay Network Tanzu with vSphere Enabled Networking (Load Balancing, Firewall etc) 1x AVI Controller 1x AVI Service Engine ","date":"2021-02-15","objectID":"/post/homelab-2021/:2:0","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"Workloads ","date":"2021-02-15","objectID":"/post/homelab-2021/:3:0","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"Common Services 2x Windows 2019 Domain Controllers + DNS 1x Windows 2019 Root CA (configured and shutdown) 1x Windows 2019 Subordinate CA 1x Windows 2019 Microsoft SQL 1x Splunk (deprecated - poorly configured) - currently utilizing the Syslog capability from Unraid 1x Ubuntu Docker (deprecated) - utilizing the Docker functionality from Unraid ","date":"2021-02-15","objectID":"/post/homelab-2021/:3:1","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"EUC WS1 UEM 1x Console/Device Services 1x Unified Access Gateway Appliance 1x Access Appliance 1x Access Connector Appliance Bunch of Windows 10 VMs Horizon 1x Connection Server 1x Composer 1x Master Image Templates Bunch of Windows 10 Templates (built using Packer) Bunch of Windows Server Templates Bunch of Ubuntu Server Templates ","date":"2021-02-15","objectID":"/post/homelab-2021/:3:2","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"Kubernetes 1x Vmware Event Broker Appliance w/ OpenFaaS VEBA Functions 1x Ubuntu 20.04 Kubernetes Control Node 2x Ubuntu 20.04 Kubernetes Compute Node ","date":"2021-02-15","objectID":"/post/homelab-2021/:3:3","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"Use cases Replicate and test Customer Scenarios Build Demo environments for Events, Customer Workshops Test new Technology and Products Learn and test new automation capabilities example - Packer and Terraform ","date":"2021-02-15","objectID":"/post/homelab-2021/:4:0","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"How it started,How it‚Äôs going They said I needed a rack for my servers\" They said I needed a rack for my servers First ESXi Install\" First ESXi Install Upgrading my first CPU\" Upgrading my first CPU Toddler trouble with the lab\" Toddler trouble with the lab When I asked my wife to assist with the cable management\" When I asked my wife to assist with the cable management Lab is now baby proofed\" Lab is now baby proofed ","date":"2021-02-15","objectID":"/post/homelab-2021/:5:0","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"Wrap Up Hopefully this gives you an idea of the costs involved and helps guide you in the decision making process - starting from procurement to install to configuration. While maintaining a homelab is a expensive ordeal and a huge undertaking in of itself, it gives me the opportunity to continuously learn as well as share those learnings broadly and widely! ‚ÄúLife doesn‚Äôt put a limit on how much you can learn, you do.‚Äù ‚Äï Matshona Dhliwayo I want to sign off by encouraging anyone that are on the edge to get started today. Please feel free to reach out with any questions or assistance and I along with the #vExpert community will be happy to assist. ","date":"2021-02-15","objectID":"/post/homelab-2021/:6:0","tags":["homelab","vExpert"],"title":"2021: What's in my homelab?","uri":"/post/homelab-2021/"},{"categories":["SDDC"],"content":"For most folks that have been involved with SDDC for some time, this is going to be a lot of basic and obvious. As a quick disclaimer, I‚Äôve to add that when i started down this path back in April, 2020(which was around the time vSphere 7.0 was released), I was (and still am) pretty new to the world of Home Lab and SDDC. As someone who has primarily worked on the application/software layer, I have been oblivious to the underlying infrastructure that made it all possible. I wanted to be educated and to learn more about the underpinnings of how Compute, Storage, Network come together to enable and drive modern innovations. I was just getting into the Kubernetes space through the VMware Event Broker Appliance Fling and was super excited about the whole Cloud Native movement! While there are a lot of easy (üò¨ thanks to the Cloud Providers, Cluster API, Kind, minikube) or simpler alternatives to get a Kubernetes cluster going, I wanted to go down the vSphere 7 rabbit hole primarily to understand VMware‚Äôs SDDC stack and its capabilities. So let‚Äôs checkout my journey‚Ä¶ ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:0:0","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Lab BOM Here is my BOM when I started this Journey to turn on the Workload Management in vSphere 7.0 - I spent a lot on Compute and minimally on storage and networking equipments. ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:1:0","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Hosts 2x Dell R620 (unsupported CPU for vSphere 7.0) CPU: 2x E5-2630 v2 Memory: 128GB RAM Storage: Model: PERC H310 Mini (for monolithic) 2x 500GB HDD 2x Dell R730 CPU: 2x E5-2620 v3 Memory: 128GB RAM Storage: Model: PERC H730 Mini 2x 500GB HDD 2x 100GB SSD ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:1:1","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"SDDC Setup vCenter 7.0 GA Mgmt: ESXi 6.7 on 2x R620 Compute: ESXi 7.0 GA on 2x R730 2 node vSAN Cluster enabled (after purchasing a couple of 100GB SSDs) ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:1:2","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Networking TP-Link (an old leftover router) routes to my ISP through a firewall Cisco 2960 Switch (mainly for VLANs) 10.10.10.0/24 - (VLAN 10) Management 10.10.20.0/24 - (VLAN 20) VM 10.10.30.0/24 - (VLAN 30) vMotion 10.10.50.0/24 - (VLAN 50) vSAN 10.10.60.0/24 - (VLAN 60) NSX Overlay/TEP 10.10.70.0/24 - (VLAN 70) NSX Uplink default route back to TP-Link As pre-requisites, Kubernetes on vSphere demanded a lot! vSphere cluster with at least 3 ESXi hosts. If you are using vSAN you need a minimum of 4 ESXi hosts Configure the vSphere cluster with high-availability (HA) enabled Configure the vSphere cluster with the Distributed Resource Scheduler (DRS) enabled - DRS automation must be set to Fully Automated mode The vSphere cluster must use shared storage such as vSAN. Shared storage is required for vSphere HA, DRS, and for storing persistent container volumes Deploy NSX-T for container networking So right away, for someone that is new to managing a SDDC (even in a homelab capacity), Workload Management was difficult to enable. However, I saw this as an opportunity to dive headfirst into SDDC and have a foundation with vSphere, vSAN and NSX stood-up to have an Enterprise-like foundation where i can then build my other capabilities for EUC. So it has to be a win-win situation, right? and.. 9 months later‚Ä¶ Enabled Workload Management on Feb 2021\" Enabled Workload Management on Feb 2021 There has been a ton of learnings from misconfigurations and unknowns along the way which I‚Äôm going to document in this blog in the hopes that it benefits anyone else that is new to this space! ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:1:3","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Challenge 1: A Shared Storage The first challenge that I tried to solve was having a shared Storage for the lab. I looked at either turning on vSAN or setting up NFS on a new server ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:2:0","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Setting up vSAN From acquiring my hardware to getting my homelab primed for vSAN, it took me some time to work my way up to enabling vSAN (major accomplishment in my mind). Since I needed Flash based cache drives I ended up buying 4x 100GB SSD drives for cache that would let me enable a 2 node vSAN. This was supposed to be fairly easy but I had challenges with vSAN nodes and the witness modes getting partitioned. It was due to the flat network I had at the time and was able to resolve it by enabling the witness node traffic on the management network. Referenced Links https://www.virtuallyghetto.com/2018/04/native-mac-learning-in-vsphere-6-7-removes-the-need-for-promiscuous-mode-for-nested-esxi.html https://kb.vmware.com/s/article/2150433?lang=en_US https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.vsan-planning.doc/GUID-03204C22-C069-4A18-AD96-26E1E1155D21.html I eventually ended up buying a Cisco 2960 Switch where I was able to setup some VLANs and get the traffic isolated properly. ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:2:1","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Setting up NFS - Unraid Even though I had the vSAN enabled, I didn‚Äôt have a lot of space that I could work with. So I promptly started to look for a proper Storage solution and ended up choosing Unraid(Thanks to the recommendation from Erick Marshall Follow) for my NFS). Needless to say I had a bunch of challenges here as well! While there are some SuperMicro options for NFS, I was in the mode of buying Dell Servers and ordered an old R510 server with amazing specs.Unfortunately it arrived broken and I had to return it I ended up getting another Dell server R720XD which didn‚Äôt have the right HBA card (Unraid requires that you expose the disks as JBOD) but had 92gb of RAM. Rather than buying a proper HBA card, I hacked around by setting up Raid 0 volumes with 1 drive each (going against all the advices on the forums) and enabled write through on each volume I thought I was set but after running some VMs for a couple of days on Unraid, i realized that the VMs were ‚Äúworking‚Äù right up until the RAM limit were reached. The 92GB of RAM were being used as cache by Unraid. I tried not to spend a lot of money but ended up getting a backplane for my R720XD so i could add 2 SSD drives for cache. I managed to screw up by ordering SFF backplane and not the LFF backplane I had üò≠ ‚Ä¶and there went my negotiated 10$ bargain on returning the cable for the right ones I finally installed the cache drives (2x 1TB SSD) and also enabled LAG on the two NICs for the peak performance that I could afford Unraid Spec: Dell R720XD CPU: 2x E5-2670 Memory: 92GB RAM Storage: Model: PERC H710 12x 3.5\" 3TB HDD 2x 2.5\" 1TB SSD ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:2:2","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Challenge 2: NSX I knew this was going to be a huge hurdle since I had NO experience with NSX, Absolutely NONE! There were two uses for NSX in my lab I needed the setup for WCP I wanted to leverage NSX load balancers for my EUC setup (UAGs, UEM, Access etc.) ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:3:0","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Attempt #1: Ubuntu File System Corruption I was able to setup NSX minimally late last year using Keith Lee‚Äôs Follow blog and then I had to take a break from Lab for mainstream EUC work. I also had to change my priorities so I could deploy an Load Balancer for the EUC lab. For a brief moment, I went down the path of taking a whole NSX-T 3.0 Install, Configure, Manage so I can setup NSX properly. Needless to say but I lost some tempo due to competing priorities and NSX Lab took a backseat in the fall of 2020. After the holidays when I looked at my whole setup , I figured I‚Äôd be better off if I seek some help! I got some assistance from Aaron Ramirez Follow and we first tried to upgrade NSX-T to 3.0.2 and when the upgrade didn‚Äôt work from the GUI, we decided to start over with NSX-T üòÖ. This time around, I wanted to leverage my Unraid and decided to install NSX w/ Unraid as the datastore and this was a bad move on my part. I‚Äôd setup Unraid to use the cache for VMs and had also setup a mover job that would move the files from cache to the HDD overnight. My theory is that the Unraid mover job somehow corrupted the Ubuntu file system. The next day after all the setup was ‚Äúcomplete‚Äù, I got a nice index out of sync error all over the NSX Manager UI Enabled Workload Management on Feb 2021\" Enabled Workload Management on Feb 2021 I couldn‚Äôt find anyone that seemed to have faced this issue before and decided to start over‚Ä¶ Referenced Links http://keithlee.ie/2018/11/20/pks-nsx-t-home-lab-part-7-nsx-t-install/ ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:3:1","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Attempt #2: NSX Cleanup and Reinstall I went through the reinstall and configured process for NSX, boldly deciding to stick with Unraid! I ended up having the same issues but got worse this time - a login loop upon boot of NSX Manager (without any user interaction) Anyone seen #NSX mgr go crazy like this? this happened after a CLI reboot (which i thought would solve a \"timed out while syncing indexes\" error). This is a brand new deployment with no VM workloads #vexpert pic.twitter.com/5EgHSn4INV ‚Äî Partheeban Kandasamy (PK) (@pkblah) January 27, 2021 I decided to start over but only because I was getting impatient and figured I‚Äôd have more luck reinstalling than having to troubleshoot the Ubuntu file system. Referenced Links https://kontainers.in/2019/09/03/how-to-cleanup-an-orphaned-esxi-host-that-is-nsx-t-prepared/ https://blog.zuthof.nl/2020/01/29/manually-removing-faulty-nsx-t-transport-nodes/ ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:3:2","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Attempt #3: Recovering a Corrupted File System Around this time, NSX-T 3.1 had been released. I had worked with Aaron to also setup AVI load balancer and since NSX-T 3.1 had some improvements for AVI LB, we decided to setup NSX-T 3.1. By now, I got wiser and installed the manager on a local datastore and the edge on Unraid (I wasn‚Äôt going to give up on my NFS easily) By 11 PM on a Friday night, I had gotten NSX-T fully configured to cover the pre-requisites for Workload Management. Next morning, the Unraid mover had moved the files from cache to HDD and one of the edge nodes started showing symptoms of disk corruptions. I wasn‚Äôt able to login to the edge node using my root or admin password. It seemed to have either reverted or gotten corrupted. I was able to find an article that helped recover the file system and restore the root password. Referenced Links https://virtusolve.home.blog/2018/11/22/nsx-manager-appliance-boot-screen-reports-unexpected-inconsistency-run-fsck-manually-dev-sda/ ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:3:3","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Challenge 3: WCP And finally it seemed like I‚Äôm all set to enabled Workload Management on my Compute cluster! Woohoo, let‚Äôs go!\" Woohoo, let‚Äôs go! ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:4:0","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Attempt 1: NSX-T 3.1 not supported My first attempt at turning on WCP ended real quick as I got the error that NSX 3.1 was not supported. In hindsight, this was obvious because I had an ‚Äúolder‚Äù version of vCenter (7.0GA). I was at peak frustration and started to doubt myself for choosing the latest version of NSX-T‚Ä¶but I not giving up yet. note to self - do not choose the latest version for your lab #homelab ‚Äî Partheeban Kandasamy (PK) (@pkblah) January 29, 2021 My options were to either rollback NSX or to upgrade vCenter to the latest version that would support NSX-T 3.1. Needless to say, I was done with NSX Configuration having done it 3 times already! ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:4:1","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Detour: Upgrade Hosts to U1 vCenter upgrade pre-check showed that I needed to upgrade 6.7 hosts to 6.7U1 and 7.0 hosts to 7.0U1. I was able to easily upgrade my Compute Cluster hosts (Dell R730) from 7.0 to 7.0U1 through the lifecycle manager. There were no VM workload on this cluster yet and that helped speed things up. As I was going to upgrade my Management hosts to 6.7U1, I wanted to see if I could sneak in a 7.0 ESXi upgrade (by bypassing the CPU check). This unfortunately didn‚Äôt work on the Dell R620 as my storage devices were not getting picked up due to a change introduced in ESXi 7.0 (Deprecation of VMKLinux). I decided to once again leave my management hosts at 6.7 U1 which I was able to upgrade manually. Unsupported VMKLinux Driver :(\" Unsupported VMKLinux Driver :( Referenced Links https://rufus.ie/ - helps create bootable USB drives from ISO https://www.virtuallyghetto.com/2020/03/homelab-considerations-for-vsphere-7.html https://blogs.vmware.com/vsphere/2019/04/what-is-the-impact-of-the-vmklinux-driver-stack-deprecation.html ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:4:2","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Detour: Upgrade vCenter to 7.0U1 With my hosts upgraded to the version required for vCenter 7.0 U1, I got busy with my first vCenter upgrade and to go along with the theme of this post, I ran into some issues with the GUI based upgrade. The upgrade through the Admin GUI would just hang at 0% for hours and (like a dummy) because the GUI estimated the finish time to 240 minutes i ended up waiting 4 hours (not once but TWO times!). After some research I ended up downloading the patch ISO and upgrading from CLI which ended up working like a charm! Alright! Now with vCenter upgraded to 7.0 U1, we should no longer see NSX issues.. Fingers crossed, let‚Äôs turn on WCP! As I proceed to turn it on, I had a major facepalm moment when I see that NSX-T is not longer required to enable Workload Management with the October Update for vCenter! VCSA 7.0 U1 doesn‚Äôt require NSX-T\" VCSA 7.0 U1 doesn‚Äôt require NSX-T Referenced Links https://docs.vmware.com/en/VMware-vSphere/6.5/com.vmware.vsphere.upgrade.doc/GUID-5FCA78EC-8637-43A4-8B28-24624E4D5EBA.html ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:4:3","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Attempt #2: Errors, More Errors I soldier on with enabling WCP with NSX-T since they provided additional features and I did learn a bunch going through the NSX-T setup! After enabling WCP I get an Error: Error configuring cluster NIC on master VM. This operation is part of API server configuration and will be retried. I couldn‚Äôt find any good lead on solving this at the time. The error seemed to indicate that the API server was not reachable but I wasn‚Äôt able to figure out why since the API server IP were on the same subnet as my Management IPs (which I was able to ping). NSX Manager on the other hand has alarms that indicated that the NSX Container Plugin (NSXP) was having issues Come on man!\" Come on man! I go through the pre-requisites again and now see that the minimum 3 nodes for the Supervisor VM cluster. I tried to get smart by adding a nested ESXi appliance to my cluster until Kalyan Krishnaswamy sent me this article about a 2 node deployment where I see copious references to William Lam‚Äôs Follow blog reminding me that there is always an easier way! Referenced Links https://www.jamesmcleod.xyz/2020/11/06/tanzu-deployment-error/ https://davidstamen.com/2020/04/10/part-4-deploying-vsphere-with-kubernetes-enabling-vsphere-with-kubernetes/ https://www.viktorious.nl/2020/05/07/vsphere-7-with-kubernetes-2-node-lab-deployment/ https://www.virtuallyghetto.com/2020/04/deploying-a-minimal-vsphere-with-kubernetes-environment.html https://community.pivotal.io/s/article/pks-1-4-cluster-creation-failure-ncp-node-agent-fails?language=en_US ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:4:4","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":["SDDC"],"content":"Attempt #3: Cross Check I am now back to my 2 node vSAN setup w/ Witness and configured the /etc/vmware/wcp/wcpsvc.yaml in VCenter to have just 2 Supervisor Control Plane VM. This time even before I turn on WCP, I go through every aspect of my setup and cross checking my setup against all the other setup instructions that I could find! As I go through the very detailed Pivotal Docs, I notice that I overlooked a few things which I promptly addressed I had a NSX Management cluster (2 manager nodes) and I didn‚Äôt setup the certificate with the VIP information I had not setup my switch/router with static routes to the egress and ingress n/w range from the Management VLAN I had also downloaded the logs from my previous WCP attempts and started to analyse them. I googled every error and eventually found this community post for the Security Context is missing error which indicated something about having consistent MTUs. While the error I observed in the logs didn‚Äôt have anything to do with MTU, the theory made sense! Reminding me how bad networking setup can screw everything up! As I go through all my vmkernel and switch settings in vCenter I noticed that I had my physical switch MTUs setup at 1900 (for NSX-T) and left most of the things in vCenter set at the default 1500. After having consistent MTUs and fixing the other items described above, I tried enabling WCP.. fingers crossed, bracing for more errors! and to my surprise‚Ä¶ Enabled Workload Management on Feb 2021\" Enabled Workload Management on Feb 2021 Referenced Links https://docs.pivotal.io/tkgi/1-9/nsxt-3-0-install.html#nsxt30-mgmt-ssl https://communities.vmware.com/t5/VMware-vSphere-Discussions/vsphere-7-with-kubenetes-alway-authorized-fail/m-p/2295879 Moment to Celebrate\" Moment to Celebrate Workload Management is a game changer as it enables you to manage Containers and VMs within the same platform! For those that are just looking to work with Kubernetes and wondering if you need to go through this rather elaborate setup to get started, the answer is NO! There are several easy methods Just K8s: Leverage the Cloud Platforms (Amazon EKS, GKE, K8s on Azure) to easily spin up a cluster. Automated vSphere 7 with K8s: Use William Lam‚Äôs automated method to spin up this entire setup or Just K8s (but manually): You could also manually provision your cluster by using my Bootstrapping Kubernetes on Ubuntu 20.04 using kubeadm w/ Calico post Thanks to the help of the community and their blogs, I was able to quickly pick up where I left off and get to this monumental moment where I have vSphere enabled with Kubernetes! Thanks again to the #vExpert community for all the invisible helping hands! üòÑ I‚Äôve tried to reference all the posts that I could recall and if there are any that I‚Äôve missed that you think could be relevant here, please comment and I‚Äôll be happy amend! While this may have been a trivial setup for some, I had to go through a steep learning curve. Hopefully this post serves as a good guide on what to avoid and look out for as you get started with Workload Management on vSphere! I want to sign off by encouraging everyone to gravitate towards learning new ideas, technologies and concepts! Dive headfirst into embracing new experiences however uncomfortable or difficult as they are and I‚Äôm positive it will not go to waste! ü§ò ","date":"2021-01-31","objectID":"/post/turning-on-wcp/:4:5","tags":["Tanzu","Kubernetes","ProjectPacific","vExpert"],"title":"Misadventures turning on Workload Management with vSphere 7","uri":"/post/turning-on-wcp/"},{"categories":null,"content":"Reflecting on 2020 Now no matter where you are located geographically or how old you are, I‚Äôm going to take a wild guess and assume that we can agree how brutal 2020 was. Personally looking back, the year 2020 has been crazy, challenging and definitely packed a punch.. A year with mixed fortunes, 2020 was a year that I‚Äôve never imagined in my lifetime. ","date":"2020-12-28","objectID":"/post/looking-back-at-2020/:0:0","tags":["Author Notes"],"title":"2020: Shattering Comfort Zones","uri":"/post/looking-back-at-2020/"},{"categories":null,"content":"Timeline Here is a look at how I experienced 2020‚Ä¶ Work Kicked off the year by taking a huge risk - being very passionate about an incubation Product #VEBA (a Fling), I attempted to try my hand at Product Management and building on top of my 2019 UW certification Jan 2020 Life We had the most snow since moving to the Pacific NorthWest in 2018 ‚òÉÔ∏è Jan 2020 Work The Product Management stint got uncertain real quick (thanks to COVID-19) as early adopters hunkered down to support their new and growing remote workforce Feb 2020 Life Along with the rest of the world, we were coming to terms with COVID-19 and this was also the last month that my wife and I worked out of an office in 2020 üè¢ Feb 2020 Work I got to learn more about Team #VEBA! As Customer interactions were limited during this time, I turned my focus towards building a strong marketing foundation for the Product - built a website and delivered a Brown Bag session. Additionally, I took inspiration from my awesome team to build my own home lab Mar 2020 Life My wife and I were adjusting to the new remote work life, challenged as we were putting in more hours working from home along with managing our almost one year old üë∂ Mar 2020 Work Marked the official end of my TAKE 3 Product Management experience only to get me more skilled at and exposing me to Product Development, Open Source, Marketing techniques including writing blogs and delivering talks. Needless to say, I continue to work with Team #VEBA in my free time Apr 2020 Life Thanks again to COVID-19, We had to forego a monumental family occasion - our first born‚Äôs very first birthday celebration üéÇ Apr 2020 Work The month of complete uncertainty as I pondered about the path ahead for my career üòñ May 2020 Life Emotionally drained from what was happening around the world (#blacklivesmatter). Almost like therapy, I ran! üèÉ ran a lot during this time and in the process got lean and healthy May 2020 Work Got the job of Customer Success Architect ‚≠ê and transitioned to a new team and a new role! Jun 2020 Life A lot of running, homelabbing and Summer activities from within the constraints of home üë®‚Äçüë©‚Äçüë¶ Jun 2020 Work Started to get into a groove with the new Team - phenomenal individuals. I also got accepted into the vExpert program for the very first time 1Ô∏è‚É£ Jul 2020 Life First road trip of 2020 to a small beach in Washington üöó Jul 2020 Work Took on an internal Project that helps define a Customer Journey to adopt VMware Technologies and also recorded my first VMworld talk üé§ Aug 2020 Life Tried to stay active by hiking, running‚Ä¶ always cautious of COVID-19 üò∑ Aug 2020 Work A lot of sleepless nights assimilating diverse inputs from a stellar yet strongly opinionated team on the ideal Customer Journey Sep 2020 Life Wild Fires üî•. Also, my father-in-law who had been helping out with our baby had to head back home to India. My wife and I panic as we juggle full-time job and full-time parenting üò± Sep 2020 Work More sleepless nights and long weeks‚Ä¶ Weeks go by fast as Customer Journey project is now starting to take shape Oct 2020 Life The MOST challenging month of 2020 as my wife and I struggle to balance full-time work and parenting üöº Oct 2020 Work Recorded my first internal org-wide talk covering the Customer Journey ü§ò Nov 2020 Life Anxious days as the US election unfolded. Our first social gathering of 2020 - Thanksgiving at our place with new and old friends üçª Nov 2020 Work Took some time off to launch cloudtekki.com. Got the idea to build this timeline (inspired from hugo timeline theme)üòÑ Dec 2020 Life Well, here we are! Turned a year wiser and took advantage of the break by getting back into reading üìö Dec 2020 ","date":"2020-12-28","objectID":"/post/looking-back-at-2020/:1:0","tags":["Author Notes"],"title":"2020: Shattering Comfort Zones","uri":"/post/looking-back-at-2020/"},{"categories":null,"content":"A lot to be thankful for 2020 also gave us a lot of things to be grateful for‚Ä¶ Lucky for the time we got to spend with our daughter - would have otherwise been a lot of time commuting, at work and very little time with her Enjoyed not having to commute everyday - 2 hours of backed up Washington traffic everyday Thankful for the Take3 Program at VMware where I learnt a variety of new skills - blogging, website design (again), video editing, building slick presentations, delivering the right message to the right audience Met with, drew inspirations and learned a lot from these awesome folks William Lam Follow - Someone who looks to unlock potential in both technology and in people. He has years of documented knowledge, passionate about technology and always learning Michael Gasch Follow - Go expert and the most thoughtful and detail oriented person I‚Äôve ever met Robert Guske Follow - The most friendliest TAM and someone who has guided me on my homelab and my blogging journey üôè Patrick Kremer Follow - The most avid learner who taught me that there is always more to be discovered and learnt Frankie Gold Follow - The most helpful person ever and also noticed that front-end work brings out her dark side üòõ Vladi Velikov Follow - Leave it to him to go from ideation to prototype in the shortest time possible Invested the time and money to build my homelab which helped me understand buying decisions, design considerations and all that comes with managing a private cloud Experienced 3 different roles in the span of a 6 months - Managing large Customers to Managing an upcoming Product to being a Technologist Grabbed the opportunity to start working with a well rounded team of Architects and made a bunch of new friends Met with more friends (over video calls) than would have normally been possible We made better acquaintances with our neighbors - they are the most helpful and the best neighbors one can ask for! Taught me to prioritize and focus on the right goals - a number of things that I promised myself that I‚Äôd do and failed - being a kubernetes contributor, running every day of 2020, building 10 whiteboards for technology concepts, adding NSX to my homelab, automating my homelab for EUC use cases Reflecting back, this year threw a curve ball at a lot of us. We are thankful to be safe, healthy, being together and minimally impacted by 2020. My heart goes out to all the first responders and those that were adversely impacted by COVID-19, the fires and all things that were 2020. ","date":"2020-12-28","objectID":"/post/looking-back-at-2020/:2:0","tags":["Author Notes"],"title":"2020: Shattering Comfort Zones","uri":"/post/looking-back-at-2020/"},{"categories":null,"content":"Challenge yourself! This year came at me with a lot of challenges and uncertainties not only at work but at home as well. With the support of my family, I took this as an opportunity to learn new skills and build new things. This year gave me an opportunity to push myself, kept me on my toes and enabled me to step outside of my comfort zone where I thrive! As you read this, hopefully you are inspired and motivated to get out of your comfort zone, to try something new, to embrace and meet any challenges head on, to keep pushing yourself to achieve your goals and dreams! ‚úä ","date":"2020-12-28","objectID":"/post/looking-back-at-2020/:3:0","tags":["Author Notes"],"title":"2020: Shattering Comfort Zones","uri":"/post/looking-back-at-2020/"},{"categories":["UEM","EUC"],"content":"What are Organization Groups in Workspace One UEM? What are the best practices with how to set Organization Groups up for our use case? Can we insert an OG between OGs? Do we have the right setup? These are very common questions on Organization Groups which is a very foundational element within Workspace ONE UEM. Over the several years of working with Workspace ONE UEM (formerly AirWatch), no one has tried to provide a complete picture of what an Organization Group is. It is taken for granted and often misused! Previously, I‚Äôve worked mostly with Enterprise Customers where UEM adoption is very mature, Organization Groups are pretty well laid out and is often already built out. Having recently transitioned to an EUC Architect, I now engage with a wide variety of Customers in different stages of Adoption and have got into many discussions and still get a lot of questions around the Organization Groups. Here is my attempt at bringing clarity on this topic. ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:0:0","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"What are they? An Organization Group is a way to organize information and form a structure for management within UEM. It can be considered similar to folders within Operation Systems or Organization Units within Active Directory. An Organization Group can have other Organization Groups as its child and Objects are anchored to an OG - Devices are enrolled at/to an OG. User accounts are created at an OG. Smart groups, Profiles, Apps are all created at an OG. I should note that in the past, new capabilities and features for reasons unclear were always made available in the ‚ÄúCustomer type‚Äù OG but those are changing now. Here are the things that make up an Organization Group An Organization Group is defined by a Name - Arbitrary name of your choice GroupId (optional) - GroupId has to be unique per environment and has to be present if a device is being enrolled in that Organization Group Type - more on this below Country - If your Location/Organization Groups are based on Geos, then assign this to the right Country Locale - Used to serve the right localized language within the UEM console Time Zone - All the time shown within the UEM console are updated to reflect this timezone (or the timezone based on your user account) From when the software was called AirWatch to now the Workspace ONE UEM, Organization Groups and Location Groups (behind the scenes) continue to remain one of those foundational design principle for a number of things.. ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:1:0","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"Object Anchor Users are created at an Organization Group and are allowed to enroll into an Organization Group. Devices are enrolled in an Organization Group. Originally used for assignments, the UEM has evolved to consolidating towards Smart Groups as the primary choice for assignments. The Smart Groups as wel Apps, Profiles, Products, Content are all anchored to the Organization Group ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:1:1","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"Multi-tenancy It was designed as a solution to enable multi-tenancy within an AirWatch hosted environment. Global ‚îú‚îÄ‚îÄ Parent1 ‚îú‚îÄ‚îÄ Parent2 ‚îú‚îÄ‚îÄ Parent3 ‚îú‚îÄ‚îÄ Parent4 ‚îú‚îÄ‚îÄ Parent5 ‚îÇ ‚îú‚îÄ‚îÄ Child1 ‚îÇ ‚îî‚îÄ‚îÄ Child2 ‚îÇ ‚îú‚îÄ‚îÄ GrandChild1 ‚îÇ ‚îú‚îÄ‚îÄ GrandChild2 ‚îÇ ‚îú‚îÄ‚îÄ GrandChild3 ‚îÇ ‚îú‚îÄ‚îÄ GrandChild4 Permissions: An Admin at an OG is able to see all configurations and Objects(Devices, Profiles, Apps etc) at that OG and only those OGs that are its child Inheritance: A Device enrolled at a OG gets all the configuration that are assigned from its hierarchy above. ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:1:2","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"Configurations Any configuration is created at an Organization Group and can be inherited or overridden at the OGs below. These configurations could span from Settings such as Directory Services, Tunnel Configuration, ABM, APNS, Google Integration etc Objects such as Profiles, Apps, Products, Content ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:1:3","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"Integrations All integrations from On-Premise components such as UAG and ACC to integrations with other WS1 Components such as Access to Partner products such as CheckPoint or Wandera. ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:1:4","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"Types of Organization Groups There are different types of Organization Groups, namely Global Customer Container Partner and others (even custom ones) Global ‚îú‚îÄ‚îÄ Customer1 ‚îú‚îÄ‚îÄ Customer2 ‚îú‚îÄ‚îÄ Container1 ‚îÇ ‚îú‚îÄ‚îÄ Container2 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Customer3 ‚îÇ ‚îî‚îÄ‚îÄ Customer4 ‚îÇ ‚îú‚îÄ‚îÄ Container3 ‚îÇ ‚îú‚îÄ‚îÄ Container4 They are generally setup like shown above and out of all the types available, the key ones to note are - ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:2:0","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"1. Global This is the default OG UEM ships with and can be considered the root Organization Group. The general assumptions are that One does not do anything in this OG (outside of a few environment-wide settings - like site urls, CDN, file storage etc.) You are always expected to always create a child OG to start configuration. ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:2:1","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"2. Customer This is the Organization Group type that is typically setup for a ‚ÄúCustomer‚Äù. The general assumptions are that A Customer OG CANNOT have another Customer OG as its child A Customer OG is usually where most integrations and configurations are setup A Customer OG is where billing information and license consumptions are usually obtained ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:2:2","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"3. Container This is, in my experience, usually every other OG type in UEM. The general assumptions are that a Container OG can have a Customer OG as its child Some Customers insert a Container b/w Global and Customer to give them some buffer for any changes Go crazy with this and the other OG types ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:2:3","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"Should I Create One? A good rule of thumb is to be ruthlessly conservative and take a minimalist approach with Organization Groups. DO NOT create one unless they are absolutely needed to solve your use case. Here are some scenarios below for reference - ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:3:0","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"Definitely NOT Assignments: Use Smart Groups instead as they offer more flexibility ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:3:1","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"Probably a Good Idea Device Ownership: You don‚Äôt need to create OGs to separate devices by ownership types unless you have some Settings/Configurations (SDK or policies for example) that are different for these device types Device Type or Model: You don‚Äôt need to unless you have teams segregated by device types (iOS team vs Windows team) ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:3:2","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"You should GEOs or Locations: Using OGs to mirror your Organization‚Äôs geographical locations is a good idea as it also enables you to setup Administrators that can manage devices in that GEO Agencies or Functions or Departments: Using OGs to reflect how your Organization is setup by groups is a good idea as they let you setup different policies Acquisitions: Using OGs to migrate devices from M\u0026A type of activities can give you the flexibility to setup policies differently and enabling a phased approach ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:3:3","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["UEM","EUC"],"content":"Take Away I could give you a few examples but they would be misleading! As the software continues to evolve, what was once a great idea for setting up Organization Groups are probably not relevant anymore. They were setup based on the requirements, the design constraints and the capabilities available in the software at that point in time. Organization groups could be adapted a number of different ways and what worked for an Industry or a Customer at a certain point in time might not work for others. Hope this gives you an understanding of what Organization Groups are, why they exists and how to use them. The bottom line is to not get too constrained by Organization Groups and conversely don‚Äôt get carried away with Organization Groups. Please provide any feedback that you have by leaving a comment and share this post using the social media icons shown below. That‚Äôs all folks! ","date":"2020-11-21","objectID":"/post/uem-organization-groups/:4:0","tags":["Unified Endpoint Management","End User Computing"],"title":"Unpacking Organization Groups in Workspace ONE UEM","uri":"/post/uem-organization-groups/"},{"categories":["ACCESS","EUC"],"content":"Evolving Access through APIs Workspace One Access is VMware‚Äôs Product Line that helps provide seamless single sign on and conditional access for Applications from End User‚Äôs client devices. It is built using technologies such as SAML (RFC 7522) OAuth (RFC 6749) and Open ID Connect If you are new to the Product, you can find more details about WorkspaceONE(WS1) Access Product Page - here Documentation - here In this post, I‚Äôm going to cover a neat trick that will help you find and build on the APIs that are available with WS1 Access. ","date":"2020-10-24","objectID":"/post/access-apis/:0:0","tags":["Access","APIs","Postman","Developer Tools"],"title":"Access: The tale of the hidden APIs","uri":"/post/access-apis/"},{"categories":["ACCESS","EUC"],"content":"First, where are the supported APIs? API Categories for WS1 Access\" API Categories for WS1 Access As a general pointer, All APIs and developer related content for VMware are available in code.vmware.com with API documentations and code samples for VMware Products. Another newer resource is developer.vmware.com (in beta as of 10/2020) which is a slick new developer documentation landing page for all VMware products. All the supported APIs for WS1 Access can be found code.vmware.com/idm as shown below Additionally, there are also some samples with WS1 APIs on code.vmware.com and one such examples is this one which assists with migrating Apps from Okta to WS1 Access ","date":"2020-10-24","objectID":"/post/access-apis/:1:0","tags":["Access","APIs","Postman","Developer Tools"],"title":"Access: The tale of the hidden APIs","uri":"/post/access-apis/"},{"categories":["ACCESS","EUC"],"content":"What if you need more? Well, why would you need more? One very obvious reason is bulk operations - I‚Äôve come across multiple requests that seek to do a bulk operations on Access such as Exporting multiple Apps Importing multiple Apps App assignment As of writing this post, these capabilities are not built into the WS1 Access UI and not available through the documented and supported APIs. Disclaimer As a word of precaution, this is definitely for those advanced users who find that Access severely lacks capabilities permitting bulk action. The blog covers undocumented APIs meaning they are not supported ","date":"2020-10-24","objectID":"/post/access-apis/:2:0","tags":["Access","APIs","Postman","Developer Tools"],"title":"Access: The tale of the hidden APIs","uri":"/post/access-apis/"},{"categories":["ACCESS","EUC"],"content":"Developer Tools: The trick to finding the right API The trick to finding these APIs is to use the Developer Tools built into Chrome or any Modern Browser. The Developer Tools is a multi-faceted tool and the one that we want provides a Fiddler like functionality allowing us to see all the HTTP requests and responses that are being processed by the browser. This article provides all the ways to get to the Developer Console on all the major Browsers. Once you have the Network Pane open on your browser, you‚Äôll also want to enable the Persist logs checkbox which will persist all the requests and responses between any redirects. Developer Tools - Persist logs\" Developer Tools - Persist logs On the Browser tab with Developer Tools launched, navigate to WS1 Access and perform the action that you are looking an API for (PRO TIP: you may want to clear the logs before you do this). The action that you performed should result in an HTTP request as shown below with the requests and the responses as shown the images below. This is the undocumented API that we are looking for. Access Search API - Request\" Access Search API - Request Access Search API - Response\" Access Search API - Response ","date":"2020-10-24","objectID":"/post/access-apis/:3:0","tags":["Access","APIs","Postman","Developer Tools"],"title":"Access: The tale of the hidden APIs","uri":"/post/access-apis/"},{"categories":["ACCESS","EUC"],"content":"Postman: Putting it into Action Now that we have out API, it‚Äôs time to put the API into action. Follow along through the screenshots to build your new API request in Postman which is a great tool and comes in really handy, especially if you are unfamiliar with coding, to make API requests and further automate them (to an extent). You can quickly build the request as I‚Äôve shown through the screenshots below. You‚Äôll need the below (which if you recall can be found in the ) API request URL HTTP method (POST, GET, PUT, DELETE etc) Authentication (Bearer, Basic etc) Request Headers (with Access, each API have their own specific Headers for Content-Type and Accept) Content-Type is the type that you are sending in the request Accept is the type that is acceptable in the response Request Query Parameters (key value pairs) Request Body (in JSON or XML format) Access Search API in Postman - Query Parameters\" Access Search API in Postman - Query Parameters Access Search API in Postman - Authentication\" Access Search API in Postman - Authentication Access Search API in Postman - Request Headers\" Access Search API in Postman - Request Headers Access Search API in Postman - Request Body\" Access Search API in Postman - Request Body Access Search API in Postman - Testing the Request\" Access Search API in Postman - Testing the Request ","date":"2020-10-24","objectID":"/post/access-apis/:4:0","tags":["Access","APIs","Postman","Developer Tools"],"title":"Access: The tale of the hidden APIs","uri":"/post/access-apis/"},{"categories":["ACCESS","EUC"],"content":"How do you Authenticate? Now, in the UI you logged in and then performed an action. This login action behind the scenes created some cookies and the subsequent requests from the same browser also sent those cookies to tell the server that the request is from an authenticated user. How do we do that in our Postman API request? While there are a few ways, I‚Äôm choosing to do this through the pre-request action which makes sure I‚Äôve a valid access token when making the request. This pre-request script is also going to save the access token as a variable that I can reuse. To enable my login without hard coding the value in the script, I‚Äôve a few environment variables setup in Postman such as vidm_url (the tenant url) vidm_un (the admin username) vidm_pw (the admin password) Access Search API in Postman - Pre-request\" Access Search API in Postman - Pre-request ","date":"2020-10-24","objectID":"/post/access-apis/:4:1","tags":["Access","APIs","Postman","Developer Tools"],"title":"Access: The tale of the hidden APIs","uri":"/post/access-apis/"},{"categories":["ACCESS","EUC"],"content":"Access Postman Collection To make it easier for you, I‚Äôve made available my postman collection for Access available below. 10/2020 - https://www.getpostman.com/collections/c50b7bb22ab87f03bc1a Login App Management (CRUD) Category Management (CRUD) App to Category mapping ","date":"2020-10-24","objectID":"/post/access-apis/:5:0","tags":["Access","APIs","Postman","Developer Tools"],"title":"Access: The tale of the hidden APIs","uri":"/post/access-apis/"},{"categories":["ACCESS","EUC"],"content":"Examples Here are a list of examples around Workspace ONe which leverage APIs to build completely new capabilities ","date":"2020-10-24","objectID":"/post/access-apis/:6:0","tags":["Access","APIs","Postman","Developer Tools"],"title":"Access: The tale of the hidden APIs","uri":"/post/access-apis/"},{"categories":["ACCESS","EUC"],"content":"1: Identity Manager Migration Tool Fling Chris Halstead has built a fling which helps export web apps from one tenant and import them into another or the same tenant. A pretty cool way to help with migration scenarios or to perform quick testing in UAT or DEV environments to mirror your PROD Apps. ","date":"2020-10-24","objectID":"/post/access-apis/:6:1","tags":["Access","APIs","Postman","Developer Tools"],"title":"Access: The tale of the hidden APIs","uri":"/post/access-apis/"},{"categories":["ACCESS","EUC"],"content":"2: WS1 SCIM Adapter Joe Rainone and Matt Williams authored the WS1 UEM SCIM Adapter which allows UEM to leverage cloud-based identity resources. Explained in much more detail in this post at virtualprivateer.com ","date":"2020-10-24","objectID":"/post/access-apis/:6:2","tags":["Access","APIs","Postman","Developer Tools"],"title":"Access: The tale of the hidden APIs","uri":"/post/access-apis/"},{"categories":["ACCESS","EUC"],"content":"Wrap up While some of these tools maybe outdated or not relevant, it gives you a great idea of how these APIs can be leveraged and hopefully this post empowers you create more solutions beyond what the product makes available. Thanks for reading! Please share and add comments if you‚Äôve found this useful! ","date":"2020-10-24","objectID":"/post/access-apis/:7:0","tags":["Access","APIs","Postman","Developer Tools"],"title":"Access: The tale of the hidden APIs","uri":"/post/access-apis/"},{"categories":["KUBERNETES"],"content":"This is not going to be an eye-opening new post about bootstrapping Kubernetes - a lot of people have done it and several blog posts already exists. I‚Äôm hoping to capture the information here to serve as an easy documentation for myself and other beginners in the area of Kubernetes and Cloud Native ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:0:0","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Getting Started As a baseline, here are my assumptions about those following along or to my future self You already have some (atleast 2) VMs or Physical hosts (any x86 computer) available You are able to install some linux distro on those VMs or Physical hosts - For the purpose of this blog, I‚Äôm going to be using Ubuntu 20.04 and providing steps for that. You can setup static IP on these servers after installing the OS. I‚Äôll provide the steps for Ubuntu 20.04 here which uses Netplan to configure network ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:1:0","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Installing Ubuntu Installing Ubuntu is pretty straight forward using the ISO. if you want to skip ahead, click here Download 20.04 live server iso - https://releases.ubuntu.com/20.04/ Machine Requirements (that i‚Äôm going to be using) CPU: 1 gigahertz or better RAM: 2 gigabyte or more Disk: 16 gigabytes Procedure (excerpt below and detailed here https://ubuntu.com/server/docs/install/step-by-step) Choose your language Update the installer (if offered) Select your keyboard layout Configure networking (the installer attempts to configure wired network interfaces via DHCP which is fine is you are using a VM and are going to create a template) Enter a hostname, username and password For storage, leave ‚Äúuse an entire disk‚Äù checked, and choose a disk to install to, then select ‚ÄúDone‚Äù on the configuration screen and confirm the install Configure a proxy or custom mirror if you have to in your network Select OpenSSH server from the list of packages to be installed You will now see log messages as the install is completed Select restart when this is complete, and log in using the username and password provided Unlike the sped up video above, this install can be a time consuming process to manually attempt on a number of servers (20-30 minutes per install). If you are using VMs, you should consider taking a snapshot and creating a template of your Base Ubuntu VM right after the install. This way when you need more copies in the future or want to start over, you have a clean Ubuntu install readily available. ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:2:0","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Clone to a VM From the Base Ubuntu template that you have, create two clones one for the master and one for the worker node. If you have gone the route of installing Ubuntu from ISO on two machines without cloning, that works too.. k8s-m1: will be the control node k8s-w1: will be the capacity node For Highly Available clusters for Kubernetes using kubeadm, refer to this kubernetes.io documentation ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:3:0","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Configuring Ubuntu for Kubernetes With our two machines now ready, we need to do some configuration that is specific to kubernetes and docker on these servers. There are not a lot of changes that needs to be done and can be done manually using multi window editing capabilities that iTerm2 offers shortcuts cmd-d and cmd-shift-d divide an existing session vertically or horizontally, respectively shortcuts cmd-shift-i and cmd-i to edit in multiple pane or go back to editing in a single pane, respectively ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:4:0","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Step 1 - Disable swapoff #disables swap immediately sudo swapoff ‚Äìa #ensures swap remains disabled after reboot sudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab There is a larger and better thread that conveys why this has to be done (or shouldn‚Äôt be) and where this falls within the scope of Kubernetes project on the things to get done - https://github.com/kubernetes/kubernetes/issues/53533 ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:4:1","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Step 2 - Setup Static IP Ubuntu networking is configured with Netplan. Succinctly, you want to edit /etc/netplan/01-netcfg.yaml to look like this network: version: 2 renderer: networkd ethernets: enp0s3: dhcp4: no addresses: [10.10.10.10/24] gateway4: 10.10.10.1 nameservers: addresses: [8.8.8.8,8.8.4.4] Once you have the Netplan config updated as required for your network, go ahead and run sudo netplan apply Here is another article that helps guide you through this change - https://linuxize.com/post/how-to-configure-static-ip-address-on-ubuntu-20-04/#configuring-static-ip-address-on-ubuntu-server ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:4:2","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Step 3 - Install Docker Next step is to install Docker which Kubernetes will be using as the container runtime. #install docker sudo apt install docker.io -y #Add your user to the docker group groupadd docker sudo usermod -aG docker $USER # enable and start docker sudo systemctl enable docker ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:4:3","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Step 4 - Prepping for Kubernetes The last item remaining to prep our servers is to download the components for Kubernetes and kubeadm as described here and shown below Steps: Add the signing key to verify k8s download Add kubernetes repos to the repo list Install these on all servers Kubeadm which is going to bootstrap the cluster Kubectl which is the CLI used to manage the cluster and Kubelet which is the component that needs to run on all machines to make kubernetes work # Install signing keys curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add # Adding kubenetes repository for APT apt-add-repository \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" # install k8s components cat \u003c\u003cEOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install kubeadm kubelet kubectl -y apt-mark hold kubelet kubeadm kubectl ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:4:4","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Bootstrap Kubernetes - Control Plane All that is left to do now is to get Kubernetes working! ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:5:0","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Step 1: Getting the Control Node ready (a.k.a master node) To do that we are going to leverage kubeadm to bootstrap kubernetes using a single command (maybe a couple..). First we want to get our Control node up and running with Kubernetes. This Control Node will run API Server (kube-apiserver): This is the API gateway (if you will) for all the components to interact with Etcd (etcd): This is the database (if you will) for all the kubernetes cluster data (from nodes to pods to everything) Scheduler (kube-scheduler): This is the backend scheduler/job(if you will) that continues to ensures the pods are running and properly distributed to all the nodes Controller Manager (kube-controller-manager): For every type of object within Kubernetes, there are Controllers available that specialize in managing that object (for example: Node, Replication, Endpoint etc) This is the noob version of describing kubernetes. If you choose to, you can always learn more at https://kubernetes.io/docs/concepts/overview/components/ We start by initializing kubernetes on the Control Node and all the info that we need the pod network CIDR at this point. Ensure that the POD CIDR does not overlap with any of your existing networking IP ranges. # using the default POD NETWORK CIDR for calico sudo kubeadm init --pod-network-cidr=192.168.0.0/16 If you have more than one NICs you will need to specify which IP address kubernetes should use for the API address (this is critical since all the other components need to talk to it for proper functionality). If you have it all working properly, you should get something similar to what i have below ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:5:1","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Step 2: Setup Kubectl for the user account Kubeadm will also provide some instructions to setup the user account with the right kube config. Run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:5:2","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Step 3: Setup Pod networking Once you have kubeconfig setup, you should now have kubectl working and be able to deploy a pod network to the cluster. I‚Äôm going to using calico for the networking and the networking policy. curl https://docs.projectcalico.org/manifests/calico-etcd.yaml -o calico.yaml In my case, since I‚Äôm using etcd for the datastore and also using TLS, i had to update the calico.yaml with the right information (as shown). I had missed/messed up this step and ended up troubleshooting for half a day. ---# Source: calico/templates/calico-etcd-secrets.yaml# The following contains k8s Secrets for use with a TLS enabled etcd cluster.# For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/apiVersion:v1kind:Secrettype:Opaquemetadata:name:calico-etcd-secretsnamespace:kube-systemdata:# Populate the following with etcd TLS configuration if desired, but leave blank if# not using TLS for etcd.# The keys below should be uncommented and the values populated with the base64# encoded contents of each file that would be associated with the TLS data.# Example command for encoding a file contents: cat \u003cfile\u003e | base64 -w 0etcd-key:\u003cbase 64 encoded etcd Key\u003eetcd-cert:\u003cbase 64 encoded etcd Certificate\u003eetcd-ca:\u003cbase 64 encoded etcd CA\u003e--- You can get the base 64 encoded values by using the commands provided below. The file locations is well known and can also be obtained by running a kubectl describe command on the etcd pod. # CA root certificate cat /etc/kubernetes/pki/etcd/ca.crt | base64 -w 0 # ETCD private and public key sudo cat /etc/kubernetes/pki/etcd/server.key | base64 -w 0 cat /etc/kubernetes/pki/etcd/server.crt | base64 -w 0 Once you have that updated you are also going to update the calico.yaml to look like the below. The calico-secrets/etcd-xx are created using the base64 encoded information that we updated above. # Source: calico/templates/calico-config.yaml# This ConfigMap is used to configure a self-hosted Calico installation.kind:ConfigMapapiVersion:v1metadata:name:calico-confignamespace:kube-systemdata:# Configure this with the location of your etcd cluster.etcd_endpoints:\"https://\u003cAPI IP\u003e:2379\"# If you're using TLS enabled etcd uncomment the following.# You must also populate the Secret below with these files.etcd_ca:\"/calico-secrets/etcd-ca\"etcd_cert:\"/calico-secrets/etcd-cert\"etcd_key:\"/calico-secrets/etcd-key\" With these edits in place, you should now be good to apply the calico.yaml to your kubernetes cluster. kubectl apply -f calico.yaml More information on how to set up calidco can be found here - https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:5:3","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["KUBERNETES"],"content":"Bootstrap Kubernetes - Capacity Nodes Once you have the Control Plane Node up and running, we can proceed to get the Capacity Node joined to the Kubernetes Cluster. To do so, you‚Äôll need to run the command provided below. If you do this immediately after running the kubeadm init on the Control Plane node, kubeadm will provide the exact command that needs to be run on the Capacity Nodes. kubeadm join --token \u003ctoken\u003e \u003ccontrol-plane-host\u003e:\u003ccontrol-plane-port\u003e --discovery-token-ca-cert-hash sha256:\u003chash\u003e In the event you need to add more nodes after and if you need to get the token and the CA certificate hash, here is how you can get them # list the tokens kubeadm token list # create a new token kubeadm token create # to get the hash openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u003e/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' After running the kubeadm join command you should get a response like below if your Capacity node was able to join the Kubernetes Controller Hope you are successfully able to follow these instructions and get a working Kubernetes cluster with Calico on Ubuntu 20.04 using kubeadm. Please provide feedback by leaving a comment and share this post using the social media icons shown below. That‚Äôs all folks! ","date":"2020-10-24","objectID":"/post/bootstrap-k8s/:6:0","tags":["Kubernetes","Automation","Ubuntu","Calico"],"title":"Beginner's Guide: Bootstrapping kubernetes on Ubuntu 20.04 using kubeadm w/ Calico","uri":"/post/bootstrap-k8s/"},{"categories":["VEBA"],"content":"Info This post was originally published in Medium - pkblah.medium.com/self-service-for-your-datacenter-part-i It doesn‚Äôt have to be Business vs IT anymore! Either sides‚Äô needs while well-intentioned may seem to be at conflict with each other.. Business aims to innovate and promote new capabilities for their consumers with the goal of improving services delivered or user experience to retain existing users or acquire new users. IT as true partners to the Business want to provide the right platform and infrastructure but need to ensure that they guaranteeing the right security guidance, conformance to standards and best practices. A key area that often exacerbates this divide is how interaction between Business and IT teams are facilitated. Traditionally and in a majority of existing Enterprises, Ticketing Systems are often the preferred way for Business to engage IT. Once a request is placed, it is then subjected to an elaborate Change Management Approval process before initiating a project requested by Business. Such processes ensure thoroughness but stunts innovation agility. Modern Cloud Platforms have challenged that approach by enabling anyone with just a credit card to spin up Servers, Services or Desktops anytime ‚Äî leading to the rise of Shadow IT. What if we bring these cloud-like capabilities to the SDDC? F. Gold and I recently delivered a VMworld Session where we built a demo of doing just that! Arm Yourself with Event-Driven Functions and Reimagine SDDC Capabilities [HCP1404] ‚Äî http://bit.ly/pksession The idea stemmed from my work with the VMware Event Broker Appliance (VEBA). VMware Event Broker Appliance deployed with OpenFaaS provides the ability to deploy functions which can be triggered in two different ways ‚Äî Event Driven ‚Äî example function that get triggered on an event to make a POST request to an HTTP endpoint is available here (works with Pagerduty, Slack, ServiceNow, ServiceDesk, Jira etc) Command/User Driven ‚Äî enabled through the function‚Äôs HTTP endpoint as explained in OpenFaaS‚Äôs blog here As Team #VEBA continues to look for ways to stretch the boundaries around capabilities unlocked through VEBA, I wanted to partner up with F. Gold to leverage the event-driven capabilities as well as incorporate the command-driven aspect of a deployed function to deliver a true self-service experience for Business Units. ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:0:0","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"So, what does the Self-Service app do? As we started putting our heads together and thinking of a good usecase to explore both the event-driven and command/user driven capabilities, the idea of a Slack bot came to mind along with some auto-remediation of issues. Here are the questions that we sought out to answer For the Command/User driven use case, What if IT enables Business Stakeholders to interact with VMware SDDC through a Slack command to ‚Äî Create a VM ‚Äî This can be useful in the early stages of enabling self-service. IT can control the size of the VM spec by possibly starting with their standard VM Spec or maybe even a lower Spec Clone a VM ‚Äî This can enable horizontal scaling of their Application or troubleshoot configuration issues by cloning their existing VMs Clone from a VM Template ‚Äî When Business teams standardize on a VM Spec and Template, IT could create a VM Template as specified by Business and validated by security. This template can be cloned by the user for their Application deployment Power cycle VMs ‚Äî At any given time, there may be a need to power cycle VMs (outside of the OS controls available). These controls can be provided to the user and also giving them the power to utilize resources efficiently Delete VM ‚ÄîIT can offload resposible clean up of SDDC resources by enabling the BU to delete VMs on demand For the event-driven use case, what if certain crises are handled automatically thus increasing operational efficiency ‚Äî Vertical Scaling a VM ‚Äî Not everyone has a clear understanding of what their VM‚Äôs spec is going to be. More often than not, to play safe, the request for VM Spec has added buffer and is unreasonably sized. What if when a VM is running low on CPU or Memory resources, a function is triggered to auto increase the VM Spec (by 1 CPU upto a predetermined max value and by 2Gb upto a predetermined max value). This will enable Business teams to start with a low or appropriately sized VM and give the assurance that VMs can scale up to handle any future loads Auto Storage DRS ‚Äî When a host datastore is running out of space, a function can be triggered to move the VM to a different datastore. {{}} ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:1:0","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Architecture To make this App a reality, we are going to build three main components First, the Gateway function called **iaasgw** written in **python**‚Äî This function is going to recieve the Slack bot command and based on the command, triage to a set of VM Lifecycle functions. This function was written in Python because of my familiarity with Python. Second, the VM Lifecycle functions written in **PowerCLI**‚Äî These functions only respond to the Gateway function and will help perform a specific action on a VM within vCenter. There are also a couple of Utility functions that i‚Äôve used for debugging and to query vCenter. These functions were first attempted to be implemented in Python but I found the PowerCLI an easy and efficient way to interact with vSphere. Lastly, the Event-driven crisis remediation functions written in Go‚Äî These functions will be triggered when an Alarm status changes tored. These functions were written in Go purely because Frankie,our Go expert, was exploring and implementing these usecases. Self-Service App Architecture with VEBA (depoyed with OpenFaaS)\" Self-Service App Architecture with VEBA (depoyed with OpenFaaS) Now, let‚Äôs look at these components in detail. ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:2:0","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"The Gateway Function (written in python) Let‚Äôs start by creating the Gateway function which is the entry point for Slack Bot. You can find the source code for this function on the pksrc/vebafn repository here ‚Äî https://github.com/pksrc/vebafn/tree/master/vm-self-service-app/python/iaasgw ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:3:0","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Slack App Setup Slack bot command setup require a publicly available HTTP endpoint. For security reasons and for most third party integrations, there is most likely a requirement to have a secure endpoint also. For my demo and as previously documented here, I used Let‚Äôs Encrypt to get a publicly trusted TLS certificate. I also needed a public IP for which i used NoIP to map the Dynamic IP from my ISP to a DNS. Publicly trusted TLS for VMware Event Broker ‚Äî here Dynamic DNS solution with NoIP ‚Äî here Pro Tip: If you are setting up a Slackbot with OpenFaaS functions, make sure to use the async-function path for the deployed function. For example if your deployed function URL is [https://pdotk.lab.net/function/veba-echo](https://pdotk.ddns.net/function/veba-echo) use [https://pdotk.lab.net/async-function/veba-echo](https://pdotk.ddns.net/function/veba-echo) as this ensures that an acknowledgement is immediately sent back to Slack and thus giving you the ability to do any processing asynchronously. ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:3:1","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Receive payload from Slack Once the Bot is setup and deployed to a channel, when a user invokes the Slack slash command /iaas, Slack makes a HTTP request to the Request URL specified during the App setup. The function would receive the following request parameters as explained here and as shown below ### # token = 'pXxmdXfxxxxxxxxx4mslg' # team_id = 'T9DXXXD7Z' # team_domain = 'pkslack' # channel_id = 'C017763BZGX' # channel_name = 'vmworld2020' # user_id = 'U9BXXXEQ1' # user_name = 'partheeban.kandasamy' # command = 'echo' # response_url = 'https://hooks.slack.com/commands/T9DiY7' # trigger_id = '1258627504981.319069523.979f734485f07cc27' # text = \"createvm\" ## Pro Tip: If you are working on a new integration or even when developing a new VEBA event-driven function and need to check the event payload, deploy any of the echo functions (here is the python version) to get a sense for the payload that the function is going to receive. ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:3:2","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Verification and Acknowledgement The first couse of action, immediately after recieving the payload from Slack, is to verify the authenticity by following the set of steps descibed in Slack‚Äôs documentation here. This ensures messages are not tampered during transit or protects against replay attacks. Pro Tip: With OpenFaaS functions, all the HTTP headers are made available within the function (container) as an environment variable prepended with Http_ and all the hyphens replaced with underscores. For example X-Slack-Signature in the HTTP request header would be available as an environment variable Http_X_Slack_Signature. Secondly, to ensure good user experience, we are going to respond back to the user with an acknowledgement. This acknowledgement will be sent to let the user know what command was invoked by making a POST request to the response_url that we got from Slack. ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:3:3","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Process Commands Now, all that is left to do is to process the commands. For this, we‚Äôll be triaging by making a HTTP POST request to a set of VM Lifecycle functions that will be deployed in the same OpenFaaS cluster. Pro Tip: These functions will be available through the endpoint http://gateway.openfaas:8080 within the kubernetes cluster. For example _https://pdotk.lab.net/function/veba-echo_ would be available at _http://gateway.openfaas:8080/function/veba-echo_ To ensure security, we‚Äôll be adding a secret key to the Slack payload which will be sent with the POST request to the VM Lifecycle function. This secret will be shared with the VM Lifecycle functions to ensure the request is indeed coming from the Gateway function. Based on the command recieved from the user, the right VM function will be invoked as shown below‚Ä¶ if('**echo**'): call [http://gateway.openfaas:8080/async-function/**powercli-echo**](http://gateway.openfaas:8080/async-function/powercli-echo') with json=slack_payload + shared_keyelif('**spawn**'): call http://gateway.openfaas:8080/async-function/**powercli-createvm**' with json=slack_payload + shared_keyelif('**clonetemplate**'): call http://gateway.openfaas:8080/function/**powercli-vmclonetemplate** with json=slack_payload + shared_keyelif('**clone**'): call http://gateway.openfaas:8080/async-function/**powercli-clonevm** with json=slack_payload + shared_keyelif('**poweron**'): call [http://gateway.openfaas:8080/async-function/**powercli-poweronvm**](http://gateway.openfaas:8080/async-function/powercli-poweronvm)with json=slack_payload + shared_keyelif('**poweroff**'): call [http://gateway.openfaas:8080/async-function/**powercli-poweroffvm**](http://gateway.openfaas:8080/async-function/powercli-poweroffvm) with json=slack_payload + shared_keyelif('**reboot**'): call http://gateway.openfaas:8080/async-function/**powercli-rebootvm** with json=slack_payload + shared_keyelif('**nuke**'): call http://gateway.openfaas:8080/async-function/**powercli-deletevm** with json=slack_payload + shared_keyelif('**transform**'): call http://gateway.openfaas:8080/async-function/**powercli-setvm** with json=slack_payload + shared_keyelif('**invoke**'): call http://gateway.openfaas:8080/async-function/**powercli-danger** with json=slack_payload + shared_keyelse: #Make a POST request to Slack response URL with an **ERROR** that the command was not found Now, let‚Äôs take a look at the VM Lifecycle functions in the next part! ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:3:4","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"The VM Lifecycle Functions (written in PowerCLI) These are the easiest functions of the lot to implement. We have set of functions implemented in PowerCLI that take a specific action within vCenter. You can find the source code for this function on the pksrc/vebafn repository here ‚Äî https://github.com/pksrc/vebafn/tree/master/vm-self-service-app/powercli These are architected to facilitate business logic addition or modification without impacting other VM functions or commands. For example, you could restrict the power cycle actions to be performed on VMs within a certain Host only, or the Create VM commands could be configured to add VMs to a certain host only. The high-level implementation logic for these functions is provided below **Psuedo Code** 1. Process function secrets or configs - this function needs the vCenter Credentials2. Process the payload received from the Gateway3. Validate that the request is indeed from the Gateway function before allowing any critical functionality - verify the presence of the shared secret4. Connect to vCenter Server and Perform the intended change such as poweron, poweroff etc.. Here is the breakdown of all the functions, their functionality and how you can invoke them once they have been deployed. ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:4:0","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Utility ‚Äî Echo the payload from the Gateway The echo function helps with troubleshooting by printing to System Out the payload received by the function. **command: /iaas echo \u003ctype something here\u003e image**: pkbu/powercli-echo ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:4:1","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Create a VM: Creates a VM of a certain specification which is configured and hardcoded to be 1 CPU, 128MB Memory and 128MB HDD. This is obviously not a realistic value for most scenarios and will have to be adjusted accordingly. **command: /iaas spawn \u003cvmname\u003e image**: pkbu/powercli-createvm ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:4:2","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Clone a VM (not covered in demo) Creates a clone of a specific VM ‚Äî I have a ‚ÄúTestVM‚Äù in my LAB which will be cloned. In a real world scenario, this could be a clone of a PROD VM to replicate issues or scale out services installed on a VM. **command: /iaas clone \u003cvmname\u003e image**: pkbu/powercli-clonevm ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:4:3","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Clone a VM from Template Creates a clone from a VM Template. Once you have an idea of the VM Spec and the Operating System that is needed, it can be made available as a VM Template which can be cloned at will by the Slack user. **command: /iaas clonetemplate \u003cvmname\u003e \u003ctemplatename\u003e image**: powercli-vmclonetemplate ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:4:4","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Power Cycle functions A set of useful powercycle functions for a VM. In a real world scenario, it helps users be responsible about managing available resources as well as giving them the ability to powercycle VMs as a troubleshooting step at will. **command: /iaas poweron \u003cvmname\u003e image**: [pkbu/powercli-poweronvm](http://gateway.openfaas:8080/async-function/powercli-poweronvm)**command: /iaas poweroff \u003cvmname\u003e image**: pkbu[/powercli-poweroffvm](http://gateway.openfaas:8080/async-function/powercli-poweroffvm)**command: /iaas reset \u003cvmname\u003e image**: pkbu[/powercli-rebootvm](http://gateway.openfaas:8080/async-function/powercli-rebootvm) ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:4:5","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Remove a VM Helps delete the VM from vCenter. Another helpful function that promotes responsible utilization of available resources. In real world, you must use caution and put in gaurd rails to prevent deleting VMs that must not be deleted. **command: /iaas nuke \u003cvmname\u003e **- **image**: pkbu[/powercli-deletevm](http://gateway.openfaas:8080/async-function/powercli-deletevm) ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:4:6","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Transform a VM (not covered in demo) This function helps change the VM hardware spec at will by the Slack user. This wasn‚Äôt covered in the VMworld demo as this was handled through an event-driven scenario that will be covered in the Go function writeup. **command: /iaas transform \u003cvmname\u003e \u003ctemplatename\u003e image**: pkbu/[powercli-setvm](http://gateway.openfaas:8080/async-function/powercli-setvm) ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:4:7","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Utility ‚Äî Run a PowerCLI command on vCenter (use caution) This is a helpful yet dangerous function as it executes a PowerCLI command as-is against a established vCenter connection and the response (if any) is sent back to Slack. I‚Äôve used this to query the status of a VM at any given point in time as shown below. **command: /iaas invoke get-vm -Name veba-test-vm| fl Id, Name, PowerState, NumCpu, CoresPerSocket, MemoryMB, VMHost | Out-string image**: pkbu[/powercli-danger](http://gateway.openfaas:8080/async-function/powercli-danger) ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:4:8","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":["VEBA"],"content":"Function Deployment We‚Äôve provided detailed steps on how to deploy these functions here in the Readme. You should be able to tweak, deploy these functions and enable VM Self Service in your organization right away! We‚Äôve covered a lot in this article and in Part II of this topic F. Gold will cover the the Event-driven Remediation functions. Enabling Self Service for your DataCenter ‚Äî Part II This required a lot of effort to become a reality! Please let us know (tweet @pkblah) how you‚Äôve used this or liked what we‚Äôve made available. Happy Eventing! ","date":"2020-10-07","objectID":"/post/self-service-for-your-datacenter/:5:0","tags":["event-driven","cloud-native"],"title":"Enabling Self Service for your DataCenter ‚Äî Part I","uri":"/post/self-service-for-your-datacenter/"},{"categories":null,"content":"Tweets by pkblah ¬†Follow @pkblah ¬†Follow @pksrc ¬†Connect on LinkedIn ¬†Send an Email ","date":"2020-10-01","objectID":"/about-guestauthor/:0:0","tags":null,"title":"About PK","uri":"/about-guestauthor/"},{"categories":null,"content":"Who is XXX? Author Name is ‚Ä¶. ","date":"2020-10-01","objectID":"/about-guestauthor/:1:0","tags":null,"title":"About PK","uri":"/about-guestauthor/"},{"categories":null,"content":"Recent Accomplishments VMworld 2020 - Arm yourself with Event Driven Functions Built the website for VMware Event Driven Automation - website Code contribution towards VMware Event Driven Automation - github vExpert 2020 VCP - DWS 2019 ","date":"2020-10-01","objectID":"/about-guestauthor/:2:0","tags":null,"title":"About PK","uri":"/about-guestauthor/"},{"categories":null,"content":"Career Currently, Staff Customer Success Architect for VMware EUC focussed on building the right solutions for everyday problems. more details here at about.pdotk.com ","date":"2020-10-01","objectID":"/about-guestauthor/:3:0","tags":null,"title":"About PK","uri":"/about-guestauthor/"},{"categories":null,"content":"Tweets by pkblah ¬†Follow @pkblah ¬†Follow @pksrc ¬†Connect on LinkedIn ¬†Email PK ","date":"2020-10-01","objectID":"/about-pk/:0:0","tags":null,"title":"About PK","uri":"/about-pk/"},{"categories":null,"content":"Who is P.K? P.K short for Partheeban Kandasamy is a Staff Customer Success Architect. P.K started his career as a Java Web Developer and after completing his Masters in Information Systems spent several years at VMware as a Technical Advocate to several high profile Customers. With over 12 years working in the intersection of Technology and People, he brings an unique combination of technical prowess, emotional intelligence and user empathy to solve difficult problems ","date":"2020-10-01","objectID":"/about-pk/:1:0","tags":null,"title":"About PK","uri":"/about-pk/"},{"categories":null,"content":"Recent Accomplishments VMworld 2020 - Arm yourself with Event Driven Functions Built the website for VMware Event Driven Automation - website Code contribution towards VMware Event Driven Automation - github vExpert 2020 VCP - DWS 2019 ","date":"2020-10-01","objectID":"/about-pk/:2:0","tags":null,"title":"About PK","uri":"/about-pk/"},{"categories":null,"content":"Career Currently, Staff Customer Success Architect for VMware EUC focussed on building the right solutions to everyday problems. more details here at about.pdotk.com ","date":"2020-10-01","objectID":"/about-pk/:3:0","tags":null,"title":"About PK","uri":"/about-pk/"},{"categories":["VEBA"],"content":"Info This post was originally published in Medium - pkblah.medium.com/publicly-trusted-tls-for-vmware-eventing-platform Vmware Event Broker Appliance (VEBA) continues to gain momentum and as Enterprise Customers start adopting the Appliance, we continue to broach Enterprise Features such as gauranteeing High Availability or the ability to upload/bootstrap the appliance with Internal CA signed or Public TLS certificates. While I had previously covered in part how the default self-signed TLS cert that is bound to OpenFaaS gateway can be updated through our documentation below, In this short post, I wanted to provide an end to end overview of obtaining a public certificate and binding it to the Ingress Gateway. VMware Event Broker Appliance - Certificates ","date":"2020-06-11","objectID":"/post/trusted-tls-for-veba/:0:0","tags":["event-driven","cloud-native"],"title":"Publicly trusted TLS for VMware Eventing Platform","uri":"/post/trusted-tls-for-veba/"},{"categories":["VEBA"],"content":"Let‚Äôs Encrypt I‚Äôm going to be using \u003cstrong\u003eLet‚Äôs Encrypt\u003c/strong\u003e which is a free, automated, open, non-profit Certificate Authority that provides digital certificates to improve internet security by lowering the barrier to entry for obtaining and binding certificates. Here‚Äôs a quick view of their rise in under 3 years ‚Äî https://letsencrypt.org/stats/\" https://letsencrypt.org/stats/ ","date":"2020-06-11","objectID":"/post/trusted-tls-for-veba/:1:0","tags":["event-driven","cloud-native"],"title":"Publicly trusted TLS for VMware Eventing Platform","uri":"/post/trusted-tls-for-veba/"},{"categories":["VEBA"],"content":"Obtaining the SSL certificate Let‚Äôs Encrypt provides a couple of methods that I‚Äôm familiar with to generate a publicly trusted TLS certificate. One mechanism assumes that you own the website and are able to standup an endpoint to prove ownership(_http://example.com/.well-known/acme-challenge/testfile_) The other mechanism assumes that you own the domain and are able to add a TXT record to prove ownership For VEBA, we are going to take the domain route. To get started, you‚Äôll need the certbot cli installed. On a mac, i was able to install the certbot using homebrew. brew install certbot Once certbot is succesfully installed, you can generate the certificate using the command below. sudo certbot certonly --manual -d path.domain.com --preferred-challenges dns After providing the affirmation to a couple of questions, certbot will request you to update the DNS to create a TXT record with _acme-challenge.path.domain.com with a generated value from certbot. This {{ }} can be done from your domain manager as shown in the screenshot. For certbot to validate the DNS TXT record under the key _acme-challenge.path.domain.com you‚Äôll provide the key ‚Äú_acme-challenge.path‚Äù for the Host and populate the TXT value with the generated string from certbot. Let‚Äôs Encrypt results\" Let‚Äôs Encrypt results Verify that the DNS record has propagated using tools like the one shown below. DNS Checker - DNS Check Propagation Tool Once verified, hit enter on the prompt for certbot to complete the certificate generation. The certs are now available under the path indicated and in the pem format which we can readily use with VMware Event Broker Appliance. Info Let‚Äôs Encrypt generates certificate that are short lived (3 months) when compared to other CAs but the generation and renewal is fairly easy and can be automated ","date":"2020-06-11","objectID":"/post/trusted-tls-for-veba/:2:0","tags":["event-driven","cloud-native"],"title":"Publicly trusted TLS for VMware Eventing Platform","uri":"/post/trusted-tls-for-veba/"},{"categories":["VEBA"],"content":"Binding Public Certificate to VEBA ","date":"2020-06-11","objectID":"/post/trusted-tls-for-veba/:3:0","tags":["event-driven","cloud-native"],"title":"Publicly trusted TLS for VMware Eventing Platform","uri":"/post/trusted-tls-for-veba/"},{"categories":["VEBA"],"content":"Assumption Access to VMware Event Broker Appliance terminal Certificates from a trusted authority pre-downloaded onto the Appliance I‚Äôve documented the steps to update the certificate without disrupting VEBA here ‚Äî https://vmweventbroker.io/kb/advanced-certificates and also made it available as a short video as shown. To provide the end to end view of how easy it is to update VEBA with new certificates, the steps are provided below. ","date":"2020-06-11","objectID":"/post/trusted-tls-for-veba/:3:1","tags":["event-driven","cloud-native"],"title":"Publicly trusted TLS for VMware Eventing Platform","uri":"/post/trusted-tls-for-veba/"},{"categories":["VEBA"],"content":"Steps Run the commands below to update the certificates cd /folder/certs/location CERT_NAME=eventrouter-tls #DO NOT CHANGE THIS KEY_FILE=\u003cprivate-key-file\u003e.pem CERT_FILE=\u003cpublic-cert-chain\u003e.pem#recreate the tls secret kubectl --kubeconfig /root/.kube/config -n vmware delete secret ${CERT_NAME} kubectl --kubeconfig /root/.kube/config -n vmware create secret tls ${CERT_NAME} --key ${KEY_FILE} --cert ${CERT_FILE}#reapply the config to take the new certificate kubectl --kubeconfig /root/.kube/config apply -f /root/config/ingressroute-gateway.yaml Once you have the certs in place, the last task is to ensure your Load Balancer or Networking rules are setup (for VMC customers, you can NAT traffic from Elastic IP to VEBA IP) to ensure the traffic flows through to VEBA. Once this is confirmed, update your domain‚Äôs record to have a A record pointing to the Load Balancer‚Äôs VIP or the Elastic IP. You should now have a VEBA endpoint that is secured with a publicly trusted TLS certificate! ","date":"2020-06-11","objectID":"/post/trusted-tls-for-veba/:3:2","tags":["event-driven","cloud-native"],"title":"Publicly trusted TLS for VMware Eventing Platform","uri":"/post/trusted-tls-for-veba/"},{"categories":["VEBA"],"content":"Automation and Beyond I wanted to dive deeper and automate parts of this and It is worth noting that automating cert generation and renewal with Let‚Äôs Encrypt has been covered by Kelsey Hightower in 2016. For customers or individuals looking to use Let‚Äôs Encrypt and if there is sufficient interest, I‚Äôll look to provide an automated method of generating Let‚Äôs Encrypt certificates along with certificate regeneration specific to VEBA. Hopefully, this provides a complete overview to obtaining a public TLS certificate and binding that to VEBA. While I‚Äôve used Let‚Äôs Encrypt here, Enterprises may want to use their existing Certificate Authorities to obtain a valid certificate and follow the steps to update the certs for VEBA. You may recall that the VMware Event Broker Appliance team works on their spare time to bring you the capabilities of event-driven automation and an extensibility platform to our VMware SDDC customers. We strive to meet market demand and build new capabilities either through features or documentation as appropriate to ensure progress. We encourage our customers to continue providing those feedbacks on where and how we can improve our product. Happy Eventing! ","date":"2020-06-11","objectID":"/post/trusted-tls-for-veba/:4:0","tags":["event-driven","cloud-native"],"title":"Publicly trusted TLS for VMware Eventing Platform","uri":"/post/trusted-tls-for-veba/"},{"categories":["VEBA"],"content":"Info This post was originally published in Medium - pkblah.medium.com/a-function-for-all-rest-apis and‚Ä¶vCenter integration with Slack, PagerDuty, ServiceNow, Zendesk, JIRA, ServiceDesk is now possible with VMware Event Broker Appliance (VEBA)! Incident Management Systems ‚Äî ‚úÖ If you don‚Äôt see how the title and the introduction are connected, read on! Let me explain how I wrote one function that makes seemingly all 3rd party system integration easy and possible! ","date":"2020-05-07","objectID":"/post/rest-api-fn/:0:0","tags":["event-driven","cloud-native"],"title":"A Function for all REST APIs","uri":"/post/rest-api-fn/"},{"categories":["VEBA"],"content":"The Why? I wrote my very first serverless function that allows you to integrate with vCenter with PagerDuty (using REST API) and this gave me a very good understanding of our User‚Äôs journey through writing a function. I leveraged this experience to brainstorm ways to improve their experience as well as ease their learning curve when it comes to writing a function! ","date":"2020-05-07","objectID":"/post/rest-api-fn/:1:0","tags":["event-driven","cloud-native"],"title":"A Function for all REST APIs","uri":"/post/rest-api-fn/"},{"categories":["VEBA"],"content":"The What? I immediately started going down two possible improvement paths The first ‚Äî Can I make it easy for our Users to get started with our functions so they don‚Äôt have to start writing from scratch? I started thinking about abstracting a few steps away for our Users and wrote this article that enables writing functions easier by making utility code, test cases, documentation prefilled and readily available! Serverless function templates Serverless function ‚Äî Templates available git clone git@github.com:pksrc/vebafn.git vebafn #get started with the basics, minimal code, lots of possibilities :) cd vebafn/python/sm-python-fn #utilities available, structure provided to customize cd vebafn/python/md-python-fn #for the sophisticated function writers - tests, logging etc cd vebafn/python/lg-python-fn The second ‚Äî If our Users wanted to integrate with another system that makes REST API available, must our Users rewrite the same code that I wrote to integrate with PagerDuty? Can I possibly make it easy for them? ","date":"2020-05-07","objectID":"/post/rest-api-fn/:2:0","tags":["event-driven","cloud-native"],"title":"A Function for all REST APIs","uri":"/post/rest-api-fn/"},{"categories":["VEBA"],"content":"The How? One function with roughly 200 lines of code and VMware Event Broker Appliance (VEBA)! Would you believe it if I said that setting up these systems took me longer than getting them to work with VEBA? Before I dive into the function itself, let‚Äôs understand the components for making a REST API call. Typically, you‚Äôll need ‚Äî metaconfig-servicenow.json\" metaconfig-servicenow.json URL: http(s)://system.example.com/v1/rest/object METHOD: usually POST but could be PUT, PATCH or DELETE etc HEADERS: API specific but usually in some sort of key value pair AUTHN: Username and Password or None in some cases BODY: API specific but takes some sort of data in json (or xml) This function aims to take these information needed to make a REST API call as configurational values and provides a way for you to pull information from the events, automatically map them to the body through mappings as shown below before firing the API request. In this very simple ServiceNow example (screenshot), the value for short_description within the body will automatically be replaced with the value of the FullFormattedMessage within the data object that we get from the vCenter Event. To integrate with more systems, all that you need is to build out this metaconfig for the different systems by providing the Rest API information and the mapping to update the request body with the event information. The function takes care of validating the event, updating the request body with the event information and making a POST request to the API. NOTE: Currently this function makes a single POST call to REST APIs to that support basic authentication, token within headers type of authentication or no authentication That‚Äôs how effortless this function along with VMware Event Broker Appliance (VEBA) makes modern integration possible with your SDDC with minimal or at times no coding. Share your feedback It would be amazing for folks to utilize this function with other systems that I‚Äôve not explored and share your results in the comments below or let me know in a tweet (@pkblah)! Happy Eventing! ","date":"2020-05-07","objectID":"/post/rest-api-fn/:3:0","tags":["event-driven","cloud-native"],"title":"A Function for all REST APIs","uri":"/post/rest-api-fn/"},{"categories":["VEBA"],"content":"-- Info This post was originally published in Medium - pkblah.medium.com/integrating-vcenter-with-pagerduty Uptime and Reliability is more important now than ever during these times when Technology and Infrastructure is enabling us fight a global pandemic with work from home policies. It is no wonder that eyes lit up when you say PagerDuty and vCenter integration! I‚Äôm going to explore how you enable this integration to automatically trigger a PagerDuty incident the minute vCenter detects something bad happens to your infrastructure! ","date":"2020-04-30","objectID":"/post/integrating-vcenter-with-pagerduty/:0:0","tags":["event-driven","cloud-native"],"title":"Integrating vCenter with PagerDuty","uri":"/post/integrating-vcenter-with-pagerduty/"},{"categories":["VEBA"],"content":"Deploy VEBA Let‚Äôs start with deploying VMware Event Broker Appliance (VEBA) which provides a event-driven Function as a Service running on a single node Kubernetes cluster delivered to you with the convenience of an appliance. Deploying the appliance takes less than 30 minutes provided you have your environment information readily available (Networking, Proxy and vCenter service credentials). ","date":"2020-04-30","objectID":"/post/integrating-vcenter-with-pagerduty/:1:0","tags":["event-driven","cloud-native"],"title":"Integrating vCenter with PagerDuty","uri":"/post/integrating-vcenter-with-pagerduty/"},{"categories":["VEBA"],"content":"PagerDuty Setup Disclaimer Skip over this if you are familiar with the first time setup process and also I‚Äôm no PagerDuty expert - There may be more than one way to do this or many considerations in setting this up for your Organization We are going to be making an API call to PagerDuty Events API to trigger an incident for a service. While most of you might have your Pagerduty configured, i‚Äôm going to provide the steps that i used to guide those that are setting it up for the first time as a POC or for testing. First I signed up for a developer PagerDuty account Sign up for your PagerDuty Developer account here Once you have your account activated or in your existing setup, you first need a team or user created, this can be done by going to Configuration ‚ÜíTeam or Configuration ‚ÜíUsers to create one or both Instructions to create users are found here and to create teams are found here Now that you have a team/user created, proceed to create an Escalation policy by going to Configuration ‚Üí Escalations to define what happens when an incident is triggered Instructions to create an escalation policy can be found here Once you have your escalations created, you are now ready to create your Service which represent an application, component, or team you wish to open incidents against. For our Service, we are going to setup the Integration to use PagerDuty Events API v2. This will give us the Integration key for us to use later within our function that we are going to deploy. Instructions to create a service can be found here To finish up and optionally, I also enabled the Slack v2 extension for my service which allows Pagerduty to send a Slack Notification when an incident is triggered. This integration also allows you to resolve an incident directly from Slack as well! Pretty neat! Instructions to set up the Slack and PagerDuty integration can be found here Alternatively, if you are looking to directly integrate your vCenter with Slack and send notifications, you can do that directly using our pre-built function that‚Äôs available here ","date":"2020-04-30","objectID":"/post/integrating-vcenter-with-pagerduty/:2:0","tags":["event-driven","cloud-native"],"title":"Integrating vCenter with PagerDuty","uri":"/post/integrating-vcenter-with-pagerduty/"},{"categories":["VEBA"],"content":"Integrating with vCenter Now that you have VEBA deployed as well as have PagerDuty setup for our integration, let‚Äôs get going with deploying our function which is readily available here. Find the summary of deployment steps below. ","date":"2020-04-30","objectID":"/post/integrating-vcenter-with-pagerduty/:3:0","tags":["event-driven","cloud-native"],"title":"Integrating vCenter with PagerDuty","uri":"/post/integrating-vcenter-with-pagerduty/"},{"categories":["VEBA"],"content":"1. Clone VEBA repository git clone https://github.com/vmware-samples/vcenter-event-broker-appliance cd vcenter-event-broker-appliance/examples/python/trigger-pagerduty-incident ","date":"2020-04-30","objectID":"/post/integrating-vcenter-with-pagerduty/:3:1","tags":["event-driven","cloud-native"],"title":"Integrating vCenter with PagerDuty","uri":"/post/integrating-vcenter-with-pagerduty/"},{"categories":["VEBA"],"content":"2. Edit the configuration file Change the configuration file pdconfig.json to add your integration key { \"routing_key\": \"\u003creplace with your routing key\u003e\", \"event_action\": \"trigger\" } ","date":"2020-04-30","objectID":"/post/integrating-vcenter-with-pagerduty/:3:2","tags":["event-driven","cloud-native"],"title":"Integrating vCenter with PagerDuty","uri":"/post/integrating-vcenter-with-pagerduty/"},{"categories":["VEBA"],"content":"3. Create the secret export OPENFAAS_URL=https://VEBA_FQDN_OR_IP faas-cli login -p VEBA_OPENFAAS_PASSWORD --tls-no-verify # now create the secret faas-cli secret create pdconfig --from-file=pdconfig.json --tls-no-verify ","date":"2020-04-30","objectID":"/post/integrating-vcenter-with-pagerduty/:3:3","tags":["event-driven","cloud-native"],"title":"Integrating vCenter with PagerDuty","uri":"/post/integrating-vcenter-with-pagerduty/"},{"categories":["VEBA"],"content":"4. Deploy the function faas-cli deploy -f stack.yml --tls-no-verify That‚Äôs it! The function by default is triggered by the VM Power Off/On Event and will immediately trigger a PagerDuty incident for our Service. If you are looking to customize the function, the detailed deployment steps provided in the README.md from the repository linked below should have all the information you are looking for. Repository Information vmware-samples/vcenter-event-broker-appliance Hopefully, this easy integration, enabled by VEBA, helps with your Organization‚Äôs SDDC Incident Management with PagerDuty. Let us know what other integrations you‚Äôd like to see and Happy Eventing! ","date":"2020-04-30","objectID":"/post/integrating-vcenter-with-pagerduty/:3:4","tags":["event-driven","cloud-native"],"title":"Integrating vCenter with PagerDuty","uri":"/post/integrating-vcenter-with-pagerduty/"},{"categories":["VEBA"],"content":"Info This post was originally published in Medium - pkblah.medium.com/serverless-function-templates You have probably have seen my other article that helps you get started writing your very first event-driven function using VMware Event Broker Appliance (VEBA) for your vCenter infrastructure. I highlight the steps taken to write my first serverless function and provide templates to help you get started quickly! Writing your first Serverless Function ‚Äî here I also wanted to build on top of my t-shirt sized templates and provide a template where users can focus on writing their core business logic, be able to test quicker and debug easily! ","date":"2020-04-29","objectID":"/post/serverless-fn-template/:0:0","tags":["event-driven","cloud-native"],"title":"Serverless function ‚Äî Quickstart Templates","uri":"/post/serverless-fn-template/"},{"categories":["VEBA"],"content":"Small function - Beginner Small ‚Äî provides a minimal template to get started from scratch complete with a readymade README.md file git clone git@github.com:pksrc/vebafn.git vebafn cd vebafn/python/sm-python-fn ‚îú‚îÄ‚îÄ README.md \u003e A descriptive documentation for your fn ‚îú‚îÄ‚îÄ handler \u003e Folder for your scripts ‚îÇ ‚îî‚îÄ‚îÄ handler.py \u003e Python file for your core fn logic ‚îî‚îÄ‚îÄ stack.yml \u003e Descriptor file for OpenFaaS deployment ","date":"2020-04-29","objectID":"/post/serverless-fn-template/:1:0","tags":["event-driven","cloud-native"],"title":"Serverless function ‚Äî Quickstart Templates","uri":"/post/serverless-fn-template/"},{"categories":["VEBA"],"content":"Medium function - Intermediate Medium ‚Äî Small + work with configurations and ready-made handle function; jump right into writing your business logic. git clone git@github.com:pksrc/vebafn.git vebafn cd vebafn/python/md-python-fn ‚îú‚îÄ‚îÄ README.MD ‚îú‚îÄ‚îÄ config.json \u003e provides any configuration for the fn ‚îú‚îÄ‚îÄ handler ‚îÇ ‚îú‚îÄ‚îÄ handler.py ‚îÇ ‚îî‚îÄ‚îÄ requirements.txt \u003e specify any libraries needed for the fn ‚îî‚îÄ‚îÄ stack.yml ","date":"2020-04-29","objectID":"/post/serverless-fn-template/:2:0","tags":["event-driven","cloud-native"],"title":"Serverless function ‚Äî Quickstart Templates","uri":"/post/serverless-fn-template/"},{"categories":["VEBA"],"content":"Large function - Expert Large ‚Äî Medium + improvements :) Here are some of the things that I wanted to do Improvements Move helper functions to another file; only have the necessary lines of code within the handler.py Make the function debuggable without having anyone touch and rebuild the code Make some settings be driven through environment variables Improve unit testing for faster function development With this in mind, I now have the below structure created and the template is available here for you to build on top of. git clone [git@github.com](mailto:git@github.com):pksrc/vebafn.git vebafn cd vebafn/python/lg-python-fn ‚îú‚îÄ‚îÄ README.MD ‚îú‚îÄ‚îÄ config.json \u003e provides any configuration for the fn ‚îú‚îÄ‚îÄ handler ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ‚îÇ ‚îú‚îÄ‚îÄ handler.py \u003e lean \u0026 focussed on business logic ‚îÇ ‚îú‚îÄ‚îÄ requirements.txt ‚îÇ ‚îî‚îÄ‚îÄ util.py \u003e util functions moved here ‚îú‚îÄ‚îÄ stack.yml \u003e new env variables added here ‚îî‚îÄ‚îÄ test.py \u003e tests have moved here Let‚Äôs explore what this looks like.. FaaSResponse class\" FaaSResponse class Our function had a few utility classes that helped sending the responses back to OpenFaaS, this was moved the util.py file to keep our handler.py clean. stack.yml for OpenFaaS\" stack.yml for OpenFaaS Logging in functions (with the templates that we are using) to stdout and stderr are both combined by default, this can be changed by setting an environment variable combine_output to false as shown in the stack.yml file. WARN if debug is enabled\" WARN if debug is enabled Logger utility class\" Logger utility class This allows us to write stderr debug messages while stdout gets sent back to OpenFaaS as the output. The function WARN users that debug is enabled and sensitive information could be logged into the console. With this in place, a logger function was added to produce colorized output in the console for readability and enhanced debuggability. test.py provided for local testing\" test.py provided for local testing The util.py is now imported into the handler.py and abstracted away for the function developer who can now focus only on building the core business logic for their usecase! I‚Äôve also moved all the test cases to outside of the package and located within the test.py and it‚Äôs been improved to run multiple test cases in one go. Make sure to edit this file to update with test cases that are relevant to what you are looking to accomplish. These changes certainly helped me write, test and debug my functions faster. While OpenFaaS has an easy process to build, deploy and invoke your function with an OpenFaaS gateway in place, the idea behind these changes and providing a local test script is to speed up the development/testing process by a few hours. ","date":"2020-04-29","objectID":"/post/serverless-fn-template/:3:0","tags":["event-driven","cloud-native"],"title":"Serverless function ‚Äî Quickstart Templates","uri":"/post/serverless-fn-template/"},{"categories":["VEBA"],"content":"See this template in action! I‚Äôve used this template and structure to build a function that can make a POST REST API call to any system (think cURL or Postman but for VEBA). This enables our users to deploy a readymade function and only have to provide the right configuration to have it work with various external systems. This function has been tested to work with Slack, Pagerduty and ServiceNow! Let me know how these templates help you innovate for your SDDC with VEBA and we can look into making templates in other languages. Happy Eventing! ","date":"2020-04-29","objectID":"/post/serverless-fn-template/:4:0","tags":["event-driven","cloud-native"],"title":"Serverless function ‚Äî Quickstart Templates","uri":"/post/serverless-fn-template/"},{"categories":["VEBA"],"content":"Info This post was originally published in Medium - pkblah.medium.com/writing-your-first-serverless-function A function is a unit of execution in the Serverless world that does one thing and one thing really well. With the current product VMware Event Broker Appliance (VEBA) that i‚Äôm managing, we aim to provide a simple solution that provides a way to execute your functions driven by vCenter events. This is a significant capability that exposes a plethora of integrations and allows seamless automation opportunities for a VMware SDDC customer. As the Product Manager, I wanted to take the opportunity to write a function not only to contribute and help the team but also as an effective way to understand one of our persona that we are targetting ‚Äî developers and especially the ones that are new to programming. This exercise of building a function gave me several insights as well as enabled me to learn more about our product and the underlying tech capabilities and constraints. Here is my account of my experience and learnings! ","date":"2020-04-23","objectID":"/post/first-veba-function/:0:0","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"},{"categories":["VEBA"],"content":"Development Setup OS: macOS Catalina IDE: VisualStudio Code Version Control: Git Language of choice: Python (no prior professional experience but have dabbled with it a few times) FaaS Platform: VEBA deployed with OpenFaaS. Alternatively, you could have OpenFaaS running on Kubernetes w/ VMware Event router container deployed to get the same functionality vCenter: Required since our functions will be driven by events generated within the SDDC. I have a homelab that i can work with for testing purposes. ","date":"2020-04-23","objectID":"/post/first-veba-function/:1:0","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"},{"categories":["VEBA"],"content":"Function Execution Flow VMware Event Broker Appliance deployed with OpenFaaS makes it easy to write functions and before we start writing the function, it‚Äôs good to get a visualization of what the function execution flow look like. The VMware Event Router streams the events from vCenter, packages those events into CloudEvent spec and invokes the appropriate function when it sees the right event. The Event is passed to OpenFaaS as a http request which then invokes the function passing the event down as a std input argument which you can read and start building your business logic. Function execution flow with vCenter Event Broker Appliance\" Function execution flow with vCenter Event Broker Appliance OpenFaaS provides readily available templates that you can use to get started and also has workshops that you can also use as a resource. ","date":"2020-04-23","objectID":"/post/first-veba-function/:2:0","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"},{"categories":["VEBA"],"content":"Writing the Function For my first function, i‚Äôm going to trigger a Pagerduty incident upon an event in vCenter. After some quick research, i was able to setup Pagerduty with a developer account, nailed down the API that i want to call and also decided on using the VM Power On/Off event to trigger an incident. ","date":"2020-04-23","objectID":"/post/first-veba-function/:3:0","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"},{"categories":["VEBA"],"content":"Hello Serverless Now before jumping into the code itself, i spent some time putting together a minimal structure to get the function working and doing something simple‚Äî such as printing the event passed down to my Function. This is what the file structure looks like.. ‚îú‚îÄ‚îÄ README.md \u003e A descriptive documentation for your function ‚îú‚îÄ‚îÄ handler \u003e Folder for your scripts ‚îÇ ‚îî‚îÄ‚îÄ handler.py \u003e Python file for your core function logic ‚îî‚îÄ‚îÄ stack.yml \u003e Descriptor file for OpenFaaS deployment A version of this is available within the examples folder of the VMware Event Broker Appliance github repository here. Note I have also made a templatized version of this available here for you to get started quickly and build on top of this. git clone git@github.com:pksrc/vebafn.git vebafncd vebafn/python/sm-python-fn Now that you have this folder cloned, let‚Äôs get started.. stack.yml ‚Äî primary descriptor for our function\" stack.yml ‚Äî primary descriptor for our function Note I‚Äôve provided screenshots below due to formatting reasons, all the code is available in the git repository here stack.yml ‚Äî this is the primary descriptor file which we are going to use to deploy the function. I made modifications as my environment required. A sample shown below.. handler.py ‚Äî parses the event and pretty prints as json to stdout\" handler.py ‚Äî parses the event and pretty prints as json to stdout handler.py‚Äî this is the core python file that should have the business logic for my function. This is what it looks like to start with.. README.md‚Äî this file contains the documentation describing the function purpose as well as steps to deploy the function With this minimal code, I proceeded to deploy this code to OpenFaaS on VEBA. You can do this by using the deployment steps provided in the README.md file here ","date":"2020-04-23","objectID":"/post/first-veba-function/:3:1","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"},{"categories":["VEBA"],"content":"Here we go.. Once I got comfortable with the deploying this simple function and seeing it execute, i proceeded with making the next iteration of changes and make our API call. These set of changes are available in the VEBA Github examples repository. A templatized version of this updated function is available here for you to get started that you can get as shown below git clone git@github.com:pksrc/vebafn.git vebafn cd vebafn/python/md-python-fn ‚îú‚îÄ‚îÄ README.MD ‚îú‚îÄ‚îÄ pdconfig.json \u003e provides any configuration for the fn ‚îú‚îÄ‚îÄ handler ‚îÇ ‚îú‚îÄ‚îÄ handler.py ‚îÇ ‚îî‚îÄ‚îÄ requirements.txt \u003e specify any libraries needed for the fn ‚îú‚îÄ‚îÄ stack.yml For my function to trigger a PagerDuty incident, i needed the following for the API call (more details here) Note API URL ‚Äî https://events.pagerduty.com/v2/enqueue Routing Key ‚Äî generated in PagerDuty Action ‚Äî ‚Äútrigger‚Äù Stack.yml ‚Äî Passing the config to the Function\" Stack.yml ‚Äî Passing the config to the Function Note Routing key and Action are specific to the API call for Pagerduty While the API URL seems static and the other configurable information can be setup to be passed as a config to my function. This is done by using the secrets keyword within the stack.yml as shown to the left. config.json ‚Äî configuration for the Function\" config.json ‚Äî configuration for the Function The config file is a json (can be any other format) with the details about the routing key and the action for the API and is structured as shown to the left. requirements.txt ‚Äî library required for the Function\" requirements.txt ‚Äî library required for the Function As we need to make http requests, we need the necessary libraries which can be specified in the requirements.txt file within the handler folder. With these files in place, I‚Äôm now ready to write my core business logic in the handler.py file using the approach below ","date":"2020-04-23","objectID":"/post/first-veba-function/:3:2","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"},{"categories":["VEBA"],"content":"Pseudocode Parse the CloudEvent provided as stdin variable to JSON Read the Configuration file - /var/openfaas/secrets/pdconfig Validate the above input data as needed and build the API body Make the API call to the PagerDuty endpoint ","date":"2020-04-23","objectID":"/post/first-veba-function/:3:3","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"},{"categories":["VEBA"],"content":"Start Coding ","date":"2020-04-23","objectID":"/post/first-veba-function/:4:0","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"},{"categories":["VEBA"],"content":"1. Parse the CloudEvent This step was already explored in the earlier steps shown above where we parse the raw text to json with exception handling to catch non-json formats ","date":"2020-04-23","objectID":"/post/first-veba-function/:4:1","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"},{"categories":["VEBA"],"content":"2. Read the Configuration file The config is made available to the function at the location /var/openfaas/secrets/\u003cconfigname\u003e within your container Here we are reading the Config file that is made available to our function under the location. I‚Äôm parsing this file and also validating that the file has the config that are required for this function to work successfully. ","date":"2020-04-23","objectID":"/post/first-veba-function/:4:2","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"},{"categories":["VEBA"],"content":"3. Validate Event and Config + Build API Body In this section, we are attempting to build the API body as well as validating if the Event has all the keys that my function needs. If the keys are not available (because the fn gets an different event than what it was coded for), then we handle the exception ","date":"2020-04-23","objectID":"/post/first-veba-function/:4:3","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"},{"categories":["VEBA"],"content":"4. Make the API call Finally, we are ready to make the API call with the URL and the body that we have generated. We are also catching any unhandled exceptions that may occur in this process. Don‚Äôt forget to close the session. Info There are a few helper classes which help with the readability of the code that i‚Äôve utilized in this function but not required. ","date":"2020-04-23","objectID":"/post/first-veba-function/:4:4","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"},{"categories":["VEBA"],"content":"Wrap Up Thats it! We now have a FaaS fn ready for testing and deployment. You can utilize the unit tests that I‚Äôve provided in the handler.py or alternatively deploy the function using the README.md file to OpenFaaS hosted on VEBA. Let me know if you‚Äôve found this helpful. In the next post, i‚Äôll provide the next iteration of changes that will help our function be more clean, debuggable and robust with the help of some util functions and best practices. Happy Eventing! ","date":"2020-04-23","objectID":"/post/first-veba-function/:5:0","tags":["event-driven","cloud-native"],"title":"Writing your first Serverless Function","uri":"/post/first-veba-function/"}]